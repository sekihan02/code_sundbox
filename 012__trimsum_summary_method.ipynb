{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea115d51-1e79-4b7c-823c-bdf893acc69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b085596-462a-4b2a-bd55-dbca435e3561",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.3.4 in /usr/local/lib/python3.10/dist-packages (1.3.4)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.4) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (1.10.13)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.4) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.4) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.4) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.4) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.4) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.4) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.4) (0.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.28.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (2.1.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (41.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: arxiv==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv==2.1.0) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from arxiv==2.1.0) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.0.10->arxiv==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv==2.1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv==2.1.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv==2.1.0) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install openai==1.2.3\n",
    "!pip install openai==1.3.4\n",
    "!pip install python-dotenv tiktoken\n",
    "!pip install pdfplumber\n",
    "!pip install arxiv==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b75d57c-a97c-4ceb-91d0-a972d89e25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str=\"{:.3f}[s]\", prefix=None, suffix=None, sep=\" \"):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b234a86-18c2-4374-996b-d816f75c4c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2403.10351v1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# 取得したいURL\n",
    "url = \"https://arxiv.org/abs/2403.10351v1\"\n",
    "\n",
    "identifier = re.search(r'/([^/]+)$', url).group(1)\n",
    "identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c5901a-0961-4108-b1c2-a482dd7e2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"処理時間を表示するクラス\n",
    "    with Timer(prefix=f'pred cv={i}'):\n",
    "        y_pred_i = predict(model, loader=test_loader)\n",
    "    \n",
    "    with Timer(prefix='fit fold={} '.format(i)):\n",
    "        clf.fit(x_train, y_train, \n",
    "                eval_set=[(x_valid, y_valid)],  \n",
    "                early_stopping_rounds=100,\n",
    "                verbose=verbose)\n",
    "\n",
    "    with Timer(prefix='fit fold={} '.format(i), verbose=500):\n",
    "        clf.fit(x_train, y_train, \n",
    "                eval_set=[(x_valid, y_valid)],  \n",
    "                early_stopping_rounds=100,\n",
    "                verbose=verbose)\n",
    "    \"\"\"\n",
    "    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611b20e1-51fe-437c-990c-e1477eed1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import re\n",
    "\n",
    "def get_arxiv_info_by_url(url):\n",
    "    # URLからタイトルと概要を取得する関数\n",
    "    # URLから論文のIDを抽出する\n",
    "    match = re.search(r'arxiv\\.org/abs/([0-9\\.v]+)', url)\n",
    "    if not match:\n",
    "        return \"URLから論文のIDを抽出できませんでした。\"\n",
    "    \n",
    "    arxiv_id = match.group(1)\n",
    "    \n",
    "    # arXivから論文情報を検索するためのクライアントと検索条件を設定\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(id_list=[arxiv_id])\n",
    "    \n",
    "    # 検索結果を取得\n",
    "    result = next(client.results(search), None)\n",
    "    \n",
    "    if result:\n",
    "        # 論文のタイトルと概要を返す\n",
    "        return {\n",
    "            \"title\": result.title,\n",
    "            \"summary\": result.summary\n",
    "        }\n",
    "    else:\n",
    "        return {\"title\": \"指定されたIDの論文が見つかりませんでした。\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ae6a04-3d42-431d-b2c8-0f55293310b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale', 'summary': \"The advent of large language models (LLMs) has significantly advanced natural\\nlanguage processing tasks like text summarization. However, their large size\\nand computational demands, coupled with privacy concerns in data transmission,\\nlimit their use in resource-constrained and privacy-centric settings. To\\novercome this, we introduce TriSum, a framework for distilling LLMs' text\\nsummarization abilities into a compact, local model. Initially, LLMs extract a\\nset of aspect-triple rationales and summaries, which are refined using a\\ndual-scoring method for quality. Next, a smaller local model is trained with\\nthese tasks, employing a curriculum learning strategy that evolves from simple\\nto complex tasks. Our method enhances local model performance on various\\nbenchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by\\n4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by\\nproviding insights into the summarization rationale.\"}\n"
     ]
    }
   ],
   "source": [
    "info = get_arxiv_info_by_url(url)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d56096-5906-4347-8ae2-99ff5383cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_pdf(link, save_path):\n",
    "    response = requests.get(link)\n",
    "    with open(save_path, 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053d02fe-3c6c-4081-b91e-a26d4e63a550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.pdf を削除しました。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files_to_remove = ['sample.pdf']\n",
    "\n",
    "for file in files_to_remove:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"{file} を削除しました。\")\n",
    "    else:\n",
    "        print(f\"{file} は存在しません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c9d732b-b1ba-4b03-8506-5d5f52f33116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download_pdf 1.823[s]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://arxiv.org/pdf/\" + identifier\n",
    "\n",
    "with Timer(prefix=\"download_pdf\"):\n",
    "    # download_pdf(url, nougat_path)\n",
    "    download_pdf(url, \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72075382-4c43-41fe-8ebf-02e8cf84a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./sample.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f160c11-8fbe-429f-a134-61a213f163bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import pdfplumber\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1007ada-7347-4576-8366-dd633a7b10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'sample.pdf'  # PDFファイルのパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c733370-865c-4865-87b6-893a11b626a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDFファイルを読み込み、テキストに変換する関数\n",
    "def pdf_to_text(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        return ''.join(page.extract_text() for page in pdf.pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8bca055-dad7-4638-8dae-93fe967b7beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_to_text 3.795[s]\n"
     ]
    }
   ],
   "source": [
    "with Timer(prefix=\"pdf_to_text\"):\n",
    "    paper_text = pdf_to_text(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ebea3f9-a4be-4da1-a6e1-4c7169c2e1d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriSum: Learning Summarization Ability from Large Language Models\n",
      "with Structured Rationale\n",
      "PengchengJiang1,CaoXiao2,ZifengWang1,ParminderBhatia2,\n",
      "JimengSun1,andJiaweiHan1\n",
      "1UniversityofIllinoisatUrbana-Champaign\n",
      "2GEHealthCare\n",
      "1{pj20, zifengw2, jimeng, hanj}@illinois.edu\n",
      "2danicaxiao@gmail.com\n",
      "Abstract Step 1: Step 2: Step 3:\n",
      "LLM Rationale Golden Rationale Local Training\n",
      "Probing Selection\n",
      "Theadventoflargelanguagemodels(LLMs) Document Prompt Document\n",
      "has significantly advanced natural language Scoring\n",
      "processingtasksliketextsummarization. How-\n",
      "ever, their large size and computational de- ? LLM MSm oda ell l\n",
      "mands, coupled with privacy concerns in\n",
      "datatransmission, limittheiruseinresource- Rationale Golden\n",
      "Ground-Truth Candidates Rationale\n",
      "constrained and privacy-centric settings. To Summary\n",
      "overcomethis,weintroduceTriSum,aframe-\n",
      "Figure1: Aconceptualdemonstrationofourthree-step\n",
      "workfordistillingLLMs’textsummarization\n",
      "framework TriSum that endows local small models\n",
      "abilitiesintoacompact,localmodel. Initially,\n",
      "withLLM’stextsummarizationcapability.\n",
      "LLMsextractasetofaspect-triplerationales\n",
      "andsummaries,whicharerefinedusingadual- However,manyexistingmethodsstruggletogen-\n",
      "scoring method for quality. Next, a smaller\n",
      "erate structured summaries (Brown et al., 2020;\n",
      "localmodelistrainedwiththesetasks,employ-\n",
      "Gekhmanetal.,2023;Liuetal.,2023). Thesestruc-\n",
      "ingacurriculumlearningstrategythatevolves\n",
      "tured summaries need to encompass essential as-\n",
      "from simple to complex tasks. Our method\n",
      "pects,keyentitiesandrelationships,andacoherent\n",
      "enhanceslocalmodelperformanceonvarious\n",
      "benchmarks(CNN/DailyMail,XSum,andClin- finalsummaryderivedfromtheseaspectsandratio-\n",
      "icalTrial), outperforming baselines by 4.5%, nales. Recentdevelopmentshaveseentheutiliza-\n",
      "8.5%,and7.4%,respectively. Italsoimproves tionofLLMstograspatext’stopicstructureand\n",
      "interpretabilitybyprovidinginsightsintothe coreideas(Vaswanietal.,2017a;Weietal.,2023),\n",
      "summarizationrationale.\n",
      "suggestingtheirpotentialingeneratingstructured\n",
      "text summaries. While rational distillation from\n",
      "1 Introduction\n",
      "LLMshasbeenemployedforNLPtaskslikeQA,\n",
      "naturallanguageunderstanding(NLU),andarith-\n",
      "Large language models (LLMs), such as GPT-3\n",
      "metic reasoning (Wang et al., 2022; Hsieh et al.,\n",
      "(Brownetal.,2020)anditssuccessors(Chowdhery\n",
      "2023; Magister et al., 2023; Ho et al., 2023), its\n",
      "etal.,2022;Touvronetal.,2023;OpenAI,2023),\n",
      "applicabilitytoabstractivetextsummarizationre-\n",
      "hasgreatlyadvancednaturallanguageprocessing\n",
      "mainsunexplored.\n",
      "tasks,includingmachinetranslation(Brantsetal.,\n",
      "2007), question-answering (QA) systems (Yang Inthisstudy,weaimtodistillLLMs’textsum-\n",
      "et al., 2019; Bao et al., 2021), and text summa- marization prowess into a more compact local\n",
      "rization (Liu and Lapata, 2019). However, due model. We enhance the transparency and inter-\n",
      "totheirsubstantialmodelsizeandcomputational pretability of this local model by incorporating\n",
      "demands, their utility can be limited in resource- elicitedrationalesfromLLMs’summarizationpro-\n",
      "constrained environments (Strubell et al., 2019). cessasadditionalguidance. Toachievethis,wein-\n",
      "Moreover,privacybecomesamajorconcernwhen troduceathree-stepframeworkTriSum(asshown\n",
      "sendingproprietarydatatoexternalLLMservices in Figure 1) involving LLM rationale probing,\n",
      "likeChatGPT. goldenrationaleselection,andlocaltraining:\n",
      "Among others, text summarization is a crucial Step 1: We first prompt vital aspect-triple ratio-\n",
      "task for transforming lengthy texts into concise nales and summaries from the input text using\n",
      "yet informative summaries (Radev et al., 2002). LLMs. This set includes essential aspects, rele-\n",
      "4202\n",
      "raM\n",
      "51\n",
      "]LC.sc[\n",
      "1v15301.3042:viXravanttriplesextractedfromthetext,andaconcise However,theresourcedemandsofLLMshave\n",
      "summarythat’stiedtotheseaspectsandtriples. limited their widespread use. Concerns over\n",
      "Step2: Next,toensurequality,weemployadual- privacywhenusingLLM-as-a-serviceAPIshave\n",
      "scoringmethodforselectinggolden(high-quality) also arisen, especially for sensitive data. This\n",
      "rationales to use in the subsequent training. This highlightstheneedformorecompactlocalmodels\n",
      "methodevaluatesthesummary’squalitybasedon that can still capture summarization abilities.\n",
      "semanticsimilarityandensurescoherentrationales To harness the summarization ability of LLMs,\n",
      "usingatopicdistribution-basedapproach. Wang et al. (2021) uses LLMs to augment labels\n",
      "Step 3: Last, we train our compact local model for headline generation, while Liu et al. (2023)\n",
      "usingacurriculumlearningapproach(Nagatsuka usedsummariescreatedbyLLMsasbenchmarks\n",
      "etal.,2021;Xuetal.,2020). Thismethodprogres- for training their local models. LLMs were\n",
      "sivelyfine-tunesthemodelbystartingwithsimpler also used to evaluate summary quality during\n",
      "tasks and gradually advancing to more complex training. However, this approach did not fully\n",
      "ones. Thisprocessenablesourmodeltogradually transferthereasoningskillsofLLMstothelocal\n",
      "incorporatetherationalizedsummarizationskills models, indicating a partial capture of LLMs’\n",
      "acquiredfromtheLLMs. summarization abilities. Also, the uncertainty of\n",
      "Ourresearchbringsthefollowingcontributions. labels generated by deep learning models may\n",
      "affectreliability.\n",
      "• WeintroduceanewapproachthatdistillsLLMs’\n",
      "abstractivetextsummarizationpowerintoasmall\n",
      "Rationale Distillation for Interpretability in\n",
      "localmodel.\n",
      "LLMs Knowledge distillation, as introduced by\n",
      "• We design a scoring mechanism to select high- Hintonetal.(2015),referstotheconceptfortrans-\n",
      "qualityrationales,whichservesasarobustbase ferringknowledgefromalargemodel(teacher)to\n",
      "fortrainingthelocalmodel. asmallerone(student)tomakedeeplearningmod-\n",
      "elsusableinresource-limitedenvironments. This\n",
      "• Throughextensiveexperimentsweshowthatin-\n",
      "ideahasbeenappliedandextendedacrossvarious\n",
      "corporatingLLM-generatedrationalesboostsour\n",
      "fields (Sanh et al., 2019; Tang et al., 2019; Jiao\n",
      "localmodel’ssummarizationperformance.\n",
      "et al., 2019; Chen et al., 2019; Lin et al., 2020;\n",
      "Wang et al., 2023). Notably, Chen et al. (2019)\n",
      "• Weenhancemodelinterpretabilitybyanalyzing\n",
      "focused on abstractive summarization, while Lin\n",
      "LLM-derived rationales, deepening our insight\n",
      "et al. (2020) emphasized extractive summariza-\n",
      "intotheirsummarizationprocesses.\n",
      "tion. The complexity of deep neural networks\n",
      "Overall,ourstudystreamlinespowerfulsumma- hasdrivenresearchtowardmakingAImodelsin-\n",
      "rizationmodelsinresource-limitedcontexts,offer- terpretable(Ribeiroetal.,2016;Doshi-Velezand\n",
      "inginsightsintoharnessingLLMs’inherentsum- Kim,2017). Rationalegenerationisanemerging\n",
      "marizationabilities. techniqueininterpretability,highlightingamodel’s\n",
      "keyreasoningsteps(ZaidanandEisner,2008;Yu\n",
      "2 RelatedWork\n",
      "et al., 2020). In knowledge distillation, rationale\n",
      "TextSummarizationusingLLMs. Transformer- generation enhances interpretability, offering in-\n",
      "based language models (Vaswani et al., 2017b) sights into the decision-making of LLMs. This\n",
      "have improved the quality of text summarization informsthedevelopmentofbetterknowledgedis-\n",
      "significantly. These models excel at capturing tillationmethods. (Wangetal.,2022)developeda\n",
      "complex relationships in long texts. Recent re- smallermodelusingLLM-generatedrationalesand\n",
      "searchhastakenthistransformerarchitecturefur- questions. Others(Shridharetal.,2023;Hoetal.,\n",
      "therforsummarizationtasks(LiuandLapata,2019; 2023;Magisteretal.,2023;Hsiehetal.,2023)used\n",
      "Lewisetal.,2019;Zhangetal.,2020;Raffeletal., LLM-producedrationalestotrainmodels,improv-\n",
      "2020), utilizing LLMs such as ChatGPT, GPT-4, ing performance and transparency in predictions,\n",
      "andPaLM(OpenAI,2023;Chowdheryetal.,2022) primarilyfortaskslikeQA,NLU,arithmeticrea-\n",
      "whichhavebillionsofparametersandaretrained soning,andextractivesummarization (Yangetal.,\n",
      "onvastamountsoftext. Theirperformancecanbe 2023). Thishasleftagapconcerningabstractive\n",
      "furtherenhancedwhenpromptedtoexecutestep- textsummarization. Tobridgethisgap, weintro-\n",
      "by-stepreasoning(Weietal.,2023). duceanaspect-triplerationalegenerationapproach,aimed at distilling the summarization prowess of foraspect-triplerationalesintrainingdata;(2)se-\n",
      "LLMs. This method consists of a procedure of lecting golden (high-quality) rationales based on\n",
      "extractingessentialaspects,pinpointingprimaryre- summaryandcoherencyscores;and(3)traininga\n",
      "lationships,andconstructingadefinitivesummary. localmodelusingacurriculumlearningapproach.\n",
      "WedetaileachstepofTriSumasfollows.\n",
      "3 Method\n",
      "3.2 Step1: LLMRationaleProbing\n",
      "3.1 OverviewofTriSum\n",
      "We introduce TriSum, an approach transferring Given a set of documents for training, our initial\n",
      "document summarization ability from an LLM stepinvolvesleveragingtheLLMtoiterativelygen-\n",
      "( 100B)toasmallLM( 1B)viarationaleprob-\n",
      "erateasetofaspect-triplerationalesalongsidetheir\n",
      "in≥ g, golden rationale sel≤ ection, and curriculum correspondingsummaries. Theobjectiveisthefol-\n",
      "learning. Here,weassumetheLLMhasreasoning lowing: first,toenabletheLLMtopinpointessen-\n",
      "abilityandcanbeusedforprompting. Beforedis- tialaspects,andsubsequently,toelaborateoneach\n",
      "cussingindetail,wedefineafewkeyconceptsand aspectusingdetailedtriples.\n",
      "notationsbelow. In this process, the auto-regressive LLM gen-\n",
      "erates both the rationale R and the summary S.\n",
      "Definition1(Aspect) An (essential) aspect α is\n",
      "We denote the length of a sequence by . The\n",
      "definedasafewwordsrepresentingadistincttopic |·|\n",
      "rationale R = (A,T) is a sequence of tokens\n",
      "inadocument.\n",
      "r ,r ,...,r ,whichiscomposedofaspectto-\n",
      "1 2 |R|\n",
      "{ }\n",
      "-Example: Inadocumentaboutclimatechange, kens a ,a ,...,a followed by triple tokens\n",
      "1 2 |A|\n",
      "{ }\n",
      "anaspectmightbe\"risingsealevels\". t ,t ,...,t , where R = A + T . Here,\n",
      "1 2 |T|\n",
      "{ } | | | | | |\n",
      "Definition2(Triple) A triple τ = s r o is a\n",
      "Arepresentsessentialaspects,andT providesde-\n",
      "structureformattingapieceoffree-tex⟨ ti| nt| o⟩ asub- tailed triples. Each a i is an individual token in\n",
      "jects,arelationr,andanobjecto. A, and each t j is an individual token in T. The\n",
      "summary S is defined as s ,s ,...,s . Each\n",
      "1 2 |S|\n",
      "-Example: Forasentence“Catseatfish.”,“Cats” { }\n",
      "token r is generated based on the document D,\n",
      "i\n",
      "isthesubject,“eat”istherelation,and“fish”is\n",
      "theground-truthsummaryS ,andthetokenspre-\n",
      "gt\n",
      "theobject,formingatriple ⟨Cats |eat |fish ⟩. viously generated, R<i = r 1,r 2,...,r\n",
      "i−1\n",
      ". The\n",
      "{ }\n",
      "Task1(AspectExtraction(AE)) Given a docu- prediction of s is contingent upon the generated\n",
      "i\n",
      "ment D, the task of aspect extraction is defined rationaleRandS<i = s ,s ,...,s :\n",
      "1 2 i−1\n",
      "{ }\n",
      "as extracting its essential aspects A (where each\n",
      "α Arepresentsanaspect)thatapproximatesthe (cid:89)u\n",
      "∈ p(R D,S ) = p(r D,S ,R<i),\n",
      "distributionp(A D). gt i gt\n",
      "| |\n",
      "| i=1\n",
      "(1)\n",
      "Task2(TripleExtraction(TE)) Given a docu- v\n",
      "(cid:89)\n",
      "ment D and its aspects A, the triple extraction p(S D,S ,R) = p(s D,S ,R,S<i).\n",
      "gt i gt\n",
      "| |\n",
      "taskisdefinedasextractingtriplesT (whereeach i=1\n",
      "τ T representsatriple)fromD,aimingtolearn\n",
      "∈ whereS denotestheground-truthsummarycor-\n",
      "thedistributionp(T D,A). gt\n",
      "| respondingtothedocumentD. Toequipourlocal\n",
      "Task3(SummaryGeneration(SG)) Given a\n",
      "model with more interpretable and high-quality\n",
      "document D, its aspect A, and the triples T,\n",
      "rationales, we prompt the LLM for n iterations,\n",
      "the task of summary generation is defined as\n",
      "whichresultsinnpairsofrationale-summary,de-\n",
      "generating a summary S that approximates the notedas R ,S n foreachdocument. Eachpair,\n",
      "distributionp(S D,A,T). { i i }i=1\n",
      "whereR = (A ,T ),servesasacandidateforthe\n",
      "| i i i\n",
      "Task4(Rationale-SummaryGeneration(RSG)) goldenrationaleselectiondescribedasfollows.\n",
      "Given a document D, the task of rationale-\n",
      "summarygenerationisdefinedasgeneratingboth 3.3 Step2: GoldenRationaleSelection\n",
      "rationale and summary that approximates the\n",
      "Giventhegeneratedcandidaterationales,wethen\n",
      "distributionp(A,T,S D).\n",
      "incorporatetwotypesofscores-SummaryScore\n",
      "|\n",
      "As illustrated in Figure 2, TriSum operates andLatentDirichletAllocation(LDA)-basedCo-\n",
      "throughthreekeysteps: (1)tappingintotheLLM herenceScoretoselectthegoldenrationales.I. LLM Rationale Probing II. Golden Rationale Selection Golden\n",
      "Rationale\n",
      "G t ( o (i a 1 f 2v s ) )e k t n s A h F : c e oa c r o d d r o eo d c ac i u cu n m hm g e ee n sn t t st o . e na t tn h id e a li g t r as o s u pg n er d- co t tu ,rn ud- rtt ehr t u rst iuh em vms eau rm dym e,a t r aey ix, lt erd dao c t t rt e ih s pe s l e ef n so tl iil nao lw t i han esg p ects S Su com rm ea r<latexit sha1_base64=\"C8nfY5aQVuXCtnPY8IUVYlPI4Hw=\">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8eK9gOaWCbbbbt0swm7G6GE/g0vHhTx6p/x5r9x2+ag1QcDj/dmmJkXJoJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqyho0FrFqh6iZ4JI1DDeCtRPFMAoFa4Wj66nfemRK81jem3HCgggHkvc5RWMl35cYCnzI7iZd3i1X3Ko7A/lLvJxUIEe9W/70ezFNIyYNFah1x3MTE2SoDKeCTUp+qlmCdIQD1rFUYsR0kM1unpAjq/RIP1a2pCEz9edEhpHW4yi0nRGaoV70puJ/Xic1/csg4zJJDZN0vqifCmJiMg2A9Lhi1IixJUgVt7cSOkSF1NiYSjYEb/Hlv6R5UvXOq6e3Z5XaVR5HEQ7gEI7BgwuowQ3UoQEUEniCF3h1UufZeXPe560FJ5/Zh19wPr4BJ2mRxQ==</latexit>ry S i S<latexit sha1_base64=\"UBu7fxod4wX5HY895vd9UvLDhxA=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8dK7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1Bqw8GHu/NMDMvSATXxnW/nMLK6tr6RnGztLW9s7tX3j9o6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8O/Pbj6g0j+WDmSToR3QoecgZNVZqNPq8X664VXcO8pd4OalAjnq//NkbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxYZUDCWNmShszVnxMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy//Ja2zqndZPb+/qNRu8jiKcATHcAoeXEEN7qAOTWAwhCd4gVdHOM/Om/O+aC04+cwh/ILz8Q0qzo27</latexit> i ~cos S<latexit sha1_base64=\"OQbU2x4KvWT7URQKZ+17rlZ6B/M=\">AAAB7XicbVBNS8NAEJ34WetX1aOXYBE8lURFPRa9eKxoP6ANZbPdtGs3u2F3IpTQ/+DFgyJe/T/e/Ddu2xy09cHA470ZZuaFieAGPe/bWVpeWV1bL2wUN7e2d3ZLe/sNo1JNWZ0qoXQrJIYJLlkdOQrWSjQjcShYMxzeTPzmE9OGK/mAo4QFMelLHnFK0EqN+27Wx3G3VPYq3hTuIvFzUoYctW7pq9NTNI2ZRCqIMW3fSzDIiEZOBRsXO6lhCaFD0mdtSyWJmQmy6bVj99gqPTdS2pZEd6r+nshIbMwoDm1nTHBg5r2J+J/XTjG6CjIukxSZpLNFUSpcVO7kdbfHNaMoRpYQqrm91aUDoglFG1DRhuDPv7xIGqcV/6Jydnderl7ncRTgEI7gBHy4hCrcQg3qQOERnuEV3hzlvDjvzsesdcnJZw7gD5zPH8Mbj0M=</latexit> gt + S<latexit sha1_base64=\"UBu7fxod4wX5HY895vd9UvLDhxA=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8dK7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1Bqw8GHu/NMDMvSATXxnW/nMLK6tr6RnGztLW9s7tX3j9o6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8O/Pbj6g0j+WDmSToR3QoecgZNVZqNPq8X664VXcO8pd4OalAjnq//NkbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxYZUDCWNmShszVnxMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy//Ja2zqndZPb+/qNRu8jiKcATHcAoeXEEN7qAOTWAwhCd4gVdHOM/Om/O+aC04+cwh/ILz8Q0qzo27</latexit> +i ~cos(<latexit sha1_base64=\"f5CgMhslH+AQ7q/v0p21NJ0oPZ8=\">AAAB6HicbVDLTgJBEOzFF+IL9ehlIjHhRHbVqEeiF4+QyCOBDZkdemFkdnYzM2tCCF/gxYPGePWTvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLR3cxvPaHSPJYPZpygH9GB5CFn1FipXu4VS27FnYOsEi8jJchQ6xW/uv2YpRFKwwTVuuO5ifEnVBnOBE4L3VRjQtmIDrBjqaQRan8yP3RKzqzSJ2GsbElD5urviQmNtB5Hge2MqBnqZW8m/ud1UhPe+BMuk9SgZItFYSqIicnsa9LnCpkRY0soU9zeStiQKsqMzaZgQ/CWX14lzfOKd1W5qF+WqrdZHHk4gVMogwfXUIV7qEEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+AG+VjLQ=</latexit> A<latexit sha1_base64=\"+896LC9lTKgd/1WPy75cgW/MTHE=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPVi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+ag1QcDj/dmmJkXJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdX/V4r1xxq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP66QmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enp3Vqld53EU4QAO4Rg8uIAa3EIdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xsPYo2p</latexit> i ⨁ T<latexit sha1_base64=\"7d4/mVP6SItKQjz/1Ta6QNeBeSI=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8eK/YI2lM120i7dbMLuRiihP8GLB0W8+ou8+W/ctjlo9cHA470ZZuYFieDauO6XU1hZXVvfKG6WtrZ3dvfK+wctHaeKYZPFIladgGoUXGLTcCOwkyikUSCwHYxvZ377EZXmsWyYSYJ+RIeSh5xRY6WHRp/3yxW36s5B/hIvJxXIUe+XP3uDmKURSsME1brruYnxM6oMZwKnpV6qMaFsTIfYtVTSCLWfzU+dkhOrDEgYK1vSkLn6cyKjkdaTKLCdETUjvezNxP+8bmrCaz/jMkkNSrZYFKaCmJjM/iYDrpAZMbGEMsXtrYSNqKLM2HRKNgRv+eW/pHVW9S6r5/cXldpNHkcRjuAYTsGDK6jBHdShCQyG8AQv8OoI59l5c94XrQUnnzmEX3A+vgEsVI28</latexit> i )<latexit sha1_base64=\"tt6//FjGc6rDwHkNg3hRwCCgBnA=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoOgl7Croh6DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5onZZqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3EZjLU=</latexit> Argmax Aspects A<latexit sha1_base64=\"H2T18AqsMYqulRgj0MofBi3iydk=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMgHsKuinqMevEY0TwgWcLsZJIMmZ1dZnqFsOQTvHhQxKtf5M2/cZLsQRMLGoqqbrq7glgKg6777eSWlldW1/LrhY3Nre2d4u5e3USJZrzGIhnpZkANl0LxGgqUvBlrTsNA8kYwvJ34jSeujYjUI45i7oe0r0RPMIpWerjunHSKJbfsTkEWiZeREmSodopf7W7EkpArZJIa0/LcGP2UahRM8nGhnRgeUzakfd6yVNGQGz+dnjomR1bpkl6kbSkkU/X3REpDY0ZhYDtDigMz703E/7xWgr0rPxUqTpArNlvUSyTBiEz+Jl2hOUM5soQyLeythA2opgxtOgUbgjf/8iKpn5a9i/LZ/XmpcpPFkYcDOIRj8OASKnAHVagBgz48wyu8OdJ5cd6dj1lrzslm9uEPnM8fr9eNag==</latexit> ⇤\n",
      "f t (o r 3r u )m t a h Wt i s t[ u hE m N m tT a hI r eT y Y . r1 e t| r iR eE vL eA dT tI rO iN p l| e sE ,N T cI oT mY p2 o] s u es e ad s ut mo m ac ro ymp oo fs e t ht eh e d og cr uo mu en nd- t. C Sco oh re eren r<latexit sha1_base64=\"M8yWSjWzrMGmQoiwPayMcmXr9gg=\">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KomKeiz24rGC/YAmlsl22y7dbMLuRiihf8OLB0W8+me8+W/ctjlo64OBx3szzMwLE8G1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqyho0FrFqh6iZ4JI1DDeCtRPFMAoFa4Wj2tRvPTGleSwfzDhhQYQDyfucorGS70sMBT5mtUmXd0tlt+LOQJaJl5My5Kh3S19+L6ZpxKShArXueG5iggyV4VSwSdFPNUuQjnDAOpZKjJgOstnNE3JqlR7px8qWNGSm/p7IMNJ6HIW2M0Iz1IveVPzP66SmfxNkXCapYZLOF/VTQUxMpgGQHleMGjG2BKni9lZCh6iQGhtT0YbgLb68TJrnFe+qcnF/Wa7e5nEU4BhO4Aw8uIYq3EEdGkAhgWd4hTcndV6cd+dj3rri5DNH8AfO5w8O+ZG1</latexit>c C iy A<latexit sha1_base64=\"+896LC9lTKgd/1WPy75cgW/MTHE=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPVi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+ag1QcDj/dmmJkXJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdX/V4r1xxq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP66QmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enp3Vqld53EU4QAO4Rg8uIAa3EIdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xsPYo2p</latexit> i L~ KD LA D<latexit sha1_base64=\"4VW0CXWdPejN3MRt4nREEtILI9I=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyqqMegHjwmYB6QLGF20knGzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dQSy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUK6AaBZdYN9wIbMUKaRgIbAaj26nffEKleSQfzDhGP6QDyfucUWOl2l23WHLL7gxkmXgZKUGGarf41elFLAlRGiao1m3PjY2fUmU4EzgpdBKNMWUjOsC2pZKGqP10duiEnFilR/qRsiUNmam/J1Iaaj0OA9sZUjPUi95U/M9rJ6Z/7adcxolByeaL+okgJiLTr0mPK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7K3mX5vHZRqtxkceThCI7hFDy4ggrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB5oFjNA=</latexit> −(<latexit sha1_base64=\"f5CgMhslH+AQ7q/v0p21NJ0oPZ8=\">AAAB6HicbVDLTgJBEOzFF+IL9ehlIjHhRHbVqEeiF4+QyCOBDZkdemFkdnYzM2tCCF/gxYPGePWTvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLR3cxvPaHSPJYPZpygH9GB5CFn1FipXu4VS27FnYOsEi8jJchQ6xW/uv2YpRFKwwTVuuO5ifEnVBnOBE4L3VRjQtmIDrBjqaQRan8yP3RKzqzSJ2GsbElD5urviQmNtB5Hge2MqBnqZW8m/ud1UhPe+BMuk9SgZItFYSqIicnsa9LnCpkRY0soU9zeStiQKsqMzaZgQ/CWX14lzfOKd1W5qF+WqrdZHHk4gVMogwfXUIV7qEEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+AG+VjLQ=</latexit> A<latexit sha1_base64=\"+896LC9lTKgd/1WPy75cgW/MTHE=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPVi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+ag1QcDj/dmmJkXJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdX/V4r1xxq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP66QmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enp3Vqld53EU4QAO4Rg8uIAa3EIdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xsPYo2p</latexit> i ⨁ T<latexit sha1_base64=\"7d4/mVP6SItKQjz/1Ta6QNeBeSI=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8eK/YI2lM120i7dbMLuRiihP8GLB0W8+ou8+W/ctjlo9cHA470ZZuYFieDauO6XU1hZXVvfKG6WtrZ3dvfK+wctHaeKYZPFIladgGoUXGLTcCOwkyikUSCwHYxvZ377EZXmsWyYSYJ+RIeSh5xRY6WHRp/3yxW36s5B/hIvJxXIUe+XP3uDmKURSsME1brruYnxM6oMZwKnpV6qMaFsTIfYtVTSCLWfzU+dkhOrDEgYK1vSkLn6cyKjkdaTKLCdETUjvezNxP+8bmrCaz/jMkkNSrZYFKaCmJjM/iYDrpAZMbGEMsXtrYSNqKLM2HRKNgRv+eW/pHVW9S6r5/cXldpNHkcRjuAYTsGDK6jBHdShCQyG8AQv8OoI59l5c94XrQUnnzmEX3A+vgEsVI28</latexit> i )<latexit sha1_base64=\"tt6//FjGc6rDwHkNg3hRwCCgBnA=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoOgl7Croh6DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5onZZqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3EZjLU=</latexit> L~ KD LA D<latexit sha1_base64=\"4VW0CXWdPejN3MRt4nREEtILI9I=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyqqMegHjwmYB6QLGF20knGzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dQSy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUK6AaBZdYN9wIbMUKaRgIbAaj26nffEKleSQfzDhGP6QDyfucUWOl2l23WHLL7gxkmXgZKUGGarf41elFLAlRGiao1m3PjY2fUmU4EzgpdBKNMWUjOsC2pZKGqP10duiEnFilR/qRsiUNmam/J1Iaaj0OA9sZUjPUi95U/M9rJ6Z/7adcxolByeaL+okgJiLTr0mPK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7K3mX5vHZRqtxkceThCI7hFDy4ggrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB5oFjNA=</latexit> Triples\n",
      "T<latexit sha1_base64=\"b6nwwZunm9JhIqHBOgHPW9uK+qY=\">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoMgHsKuinoMevEYMS9IljA76SRDZmeXmVkhLPkELx4U8eoXefNvnCR70MSChqKqm+6uIBZcG9f9dnIrq2vrG/nNwtb2zu5ecf+goaNEMayzSESqFVCNgkusG24EtmKFNAwENoPR3dRvPqHSPJI1M47RD+lA8j5n1FjpsdY96xZLbtmdgSwTLyMlyFDtFr86vYglIUrDBNW67bmx8VOqDGcCJ4VOojGmbEQH2LZU0hC1n85OnZATq/RIP1K2pCEz9fdESkOtx2FgO0NqhnrRm4r/ee3E9G/8lMs4MSjZfFE/EcREZPo36XGFzIixJZQpbm8lbEgVZcamU7AheIsvL5PGedm7Kl88XJYqt1kceTiCYzgFD66hAvdQhTowGMAzvMKbI5wX5935mLfmnGzmEP7A+fwBzMmNfQ==</latexit>\n",
      "⇤\n",
      "template Ground-truth III. Curriculum Learning (Local Training) Curriculum\n",
      "Document\n",
      "Summary\n",
      "D<latexit sha1_base64=\"4VW0CXWdPejN3MRt4nREEtILI9I=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyqqMegHjwmYB6QLGF20knGzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dQSy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUK6AaBZdYN9wIbMUKaRgIbAaj26nffEKleSQfzDhGP6QDyfucUWOl2l23WHLL7gxkmXgZKUGGarf41elFLAlRGiao1m3PjY2fUmU4EzgpdBKNMWUjOsC2pZKGqP10duiEnFilR/qRsiUNmam/J1Iaaj0OA9sZUjPUi95U/M9rJ6Z/7adcxolByeaL+okgJiLTr0mPK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7K3mX5vHZRqtxkceThCI7hFDy4ggrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB5oFjNA=</latexit> S<latexit sha1_base64=\"OQbU2x4KvWT7URQKZ+17rlZ6B/M=\">AAAB7XicbVBNS8NAEJ34WetX1aOXYBE8lURFPRa9eKxoP6ANZbPdtGs3u2F3IpTQ/+DFgyJe/T/e/Ddu2xy09cHA470ZZuaFieAGPe/bWVpeWV1bL2wUN7e2d3ZLe/sNo1JNWZ0qoXQrJIYJLlkdOQrWSjQjcShYMxzeTPzmE9OGK/mAo4QFMelLHnFK0EqN+27Wx3G3VPYq3hTuIvFzUoYctW7pq9NTNI2ZRCqIMW3fSzDIiEZOBRsXO6lhCaFD0mdtSyWJmQmy6bVj99gqPTdS2pZEd6r+nshIbMwoDm1nTHBg5r2J+J/XTjG6CjIukxSZpLNFUSpcVO7kdbfHNaMoRpYQqrm91aUDoglFG1DRhuDPv7xIGqcV/6Jydnderl7ncRTgEI7gBHy4hCrcQg3qQOERnuEV3hzlvDjvzsesdcnJZw7gD5zPH8Mbj0M=</latexit> gt Document Document AE AE AE\n",
      "Document Aspects\n",
      "Small TE TE TE\n",
      "LLM Document Aspects Triples Model\n",
      "𝒏runs SG SG SG RSG\n",
      "MSm oda ell l MSm oda ell l MSm oda ell l Aspects <latexit sha1_base64=\"lVIHt7fdSiG9i+tZlZMWvhdtbg4=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj26nfeuLaiFg94jjhfkQHSoSCUbTSA/bcXrniVt0ZyDLxclKBHPVe+avbj1kacYVMUmM6npugn1GNgkk+KXVTwxPKRnTAO5YqGnHjZ7NTJ+TEKn0SxtqWQjJTf09kNDJmHAW2M6I4NIveVPzP66QYXvuZUEmKXLH5ojCVBGMy/Zv0heYM5dgSyrSwtxI2pJoytOmUbAje4svLpHlW9S6r5/cXldpNHkcRjuAYTsGDK6jBHdShAQwG8Ayv8OZI58V5dz7mrQUnnzmEP3A+fwAGsI2j</latexit>t0 <latexit sha1_base64=\"UgLHzPwgdhFI3nHU6cfflvIRt9A=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj26nfeuLaiFg94jjhfkQHSoSCUbTSA/a8XrniVt0ZyDLxclKBHPVe+avbj1kacYVMUmM6npugn1GNgkk+KXVTwxPKRnTAO5YqGnHjZ7NTJ+TEKn0SxtqWQjJTf09kNDJmHAW2M6I4NIveVPzP66QYXvuZUEmKXLH5ojCVBGMy/Zv0heYM5dgSyrSwtxI2pJoytOmUbAje4svLpHlW9S6r5/cXldpNHkcRjuAYTsGDK6jBHdShAQwG8Ayv8OZI58V5dz7mrQUnnzmEP3A+fwAINI2k</latexit>t1 <latexit sha1_base64=\"j9ebr9sZ/dcxq6WnsP4YZFzrOvY=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Vj04rGi/YA2lM120y7dbMLuRCihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LR7dRvPXFtRKwecZxwP6IDJULBKFrpAXvVXqnsVtwZyDLxclKGHPVe6avbj1kacYVMUmM6npugn1GNgkk+KXZTwxPKRnTAO5YqGnHjZ7NTJ+TUKn0SxtqWQjJTf09kNDJmHAW2M6I4NIveVPzP66QYXvuZUEmKXLH5ojCVBGMy/Zv0heYM5dgSyrSwtxI2pJoytOkUbQje4svLpFmteJeV8/uLcu0mj6MAx3ACZ+DBFdTgDurQAAYDeIZXeHOk8+K8Ox/z1hUnnzmCP3A+fwAJuI2l</latexit>t2 <latexit sha1_base64=\"LA2Nw+dC4y2KGZmjBVIEzV21eNE=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWj26nfeuLaiFg94jjhfkQHSoSCUbTSA/bOe+WKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+ZZ1busnt9fVGo3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AELPI2m</latexit>t3 <latexit sha1_base64=\"FqD73yY/uCIyhxLAcVziBfXK4Z8=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0m0qMeiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1Bqw8GHu/NMDMvSKQw6LpfTmFldW19o7hZ2tre2d0r7x+0TJxqxpsslrHuBNRwKRRvokDJO4nmNAokbwfjm5nffuTaiFg94CThfkSHSoSCUbTSPfZr/XLFrbpzkL/Ey0kFcjT65c/eIGZpxBUySY3pem6CfkY1Cib5tNRLDU8oG9Mh71qqaMSNn81PnZITqwxIGGtbCslc/TmR0ciYSRTYzojiyCx7M/E/r5tieOVnQiUpcsUWi8JUEozJ7G8yEJozlBNLKNPC3krYiGrK0KZTsiF4yy//Ja2zqndRPb+rVerXeRxFOIJjOAUPLqEOt9CAJjAYwhO8wKsjnWfnzXlftBacfOYQfsH5+AYMwI2n</latexit>t4\n",
      "Triples\n",
      "Aspects Triples Summary Summary [<latexit sha1_base64=\"/7VFt/C5SuLeIRRVKoqyT+RxbPk=\">AAACInicbZDLSgMxFIYzXmu9jbp0EyyCCykzbfG2KrpxWcFeoB2GTJq2oZkLyRmhDH0WN76KGxeKuhJ8GDPtULT1QOD//pNDcn4vElyBZX0ZS8srq2vruY385tb2zq65t99QYSwpq9NQhLLlEcUED1gdOAjWiiQjvidY0xvepP3mA5OKh8E9jCLm+KQf8B6nBLTlmpdtcK1TDK7tXOFOJ6/RTrE0w1KK5RmWU6xodM2CVbQmhReFnYkCyqrmmh+dbkhjnwVABVGqbVsROAmRwKlg43wnViwidEj6rK1lQHymnGSy4hgfa6eLe6HUJwA8cX9PJMRXauR7+qZPYKDme6n5X68dQ+/CSXgQxcACOn2oFwsMIU7zwl0uGQUx0oJQyfVfMR0QSSjoVPM6BHt+5UXRKBXts2L5rlKoXmdx5NAhOkInyEbnqIpuUQ3VEUWP6Bm9ojfjyXgx3o3P6dUlI5s5QH/K+P4BrSSe0w==</latexit>t0,t1]:singular-task learning\n",
      "Aspects A<latexit sha1_base64=\"+896LC9lTKgd/1WPy75cgW/MTHE=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPVi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+ag1QcDj/dmmJkXJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6mfqtR1Sax/LBjBP0IzqQPOSMGivdX/V4r1xxq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP66QmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enp3Vqld53EU4QAO4Rg8uIAa3EIdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xsPYo2p</latexit> i Triples T<latexit sha1_base64=\"7d4/mVP6SItKQjz/1Ta6QNeBeSI=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8eK/YI2lM120i7dbMLuRiihP8GLB0W8+ou8+W/ctjlo9cHA470ZZuYFieDauO6XU1hZXVvfKG6WtrZ3dvfK+wctHaeKYZPFIladgGoUXGLTcCOwkyikUSCwHYxvZ377EZXmsWyYSYJ+RIeSh5xRY6WHRp/3yxW36s5B/hIvJxXIUe+XP3uDmKURSsME1brruYnxM6oMZwKnpV6qMaFsTIfYtVTSCLWfzU+dkhOrDEgYK1vSkLn6cyKjkdaTKLCdETUjvezNxP+8bmrCaz/jMkkNSrZYFKaCmJjM/iYDrpAZMbGEMsXtrYSNqKLM2HRKNgRv+eW/pHVW9S6r5/cXldpNHkcRjuAYTsGDK6jBHdShCQyG8AQv8OoI59l5c94XrQUnnzmEX3A+vgEsVI28</latexit> i Summary S<latexit sha1_base64=\"UBu7fxod4wX5HY895vd9UvLDhxA=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU1GPRi8dK7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1Bqw8GHu/NMDMvSATXxnW/nMLK6tr6RnGztLW9s7tX3j9o6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8O/Pbj6g0j+WDmSToR3QoecgZNVZqNPq8X664VXcO8pd4OalAjnq//NkbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxYZUDCWNmShszVnxMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy//Ja2zqndZPb+/qNRu8jiKcATHcAoeXEEN7qAOTWAwhCd4gVdHOM/Om/O+aC04+cwh/ILz8Q0qzo27</latexit> i ExA t (rs Aap Ece t )c it o n ExT t (rr Tai Ep ctl )e io n GS eu nm (e Srm Ga )ta ir oy n GR S ea u nt mi eo rmn aa ta il r oe y n- [ [ [t t t1 2 3, , ,t t t2 3 4] ] ]: : :c c jo oon in nc c tu u r r ler r ae en n rnt t il l ne ea a gr rn ni in ng g ( (e laa tr el )y)\n",
      "(RSG)\n",
      "Figure 2: Distilling text summarization ability from LLM to local model using TriSum. Step 1. LLM\n",
      "RationaleProbing: Employingatemplate-basedpromptincorporatingthegivendocumentandground-truth\n",
      "summary,weengageanLLMtogenerateasetofnstep-by-steprationalesacrossniterations. Step2. Golden\n",
      "RationaleSelection: Weleveragesummaryandcoherencyscorestometiculouslychoosehigh-qualitytraining\n",
      "rationales,enhancingthetrainingdataset. Step3. CurriculumLearning: Weimplementacurriculumlearning\n",
      "strategytotrainourcompactsmallmodelwithrationalizedsummarizationabilityfromeasytochallengingtasks.\n",
      "Summary Score. For each rationale R in the oftopicsk. Itisimportanttoclarifythatthetopics\n",
      "i\n",
      "candidates R ,S n , suppose Rˆ , Sˆ, and Sˆ identifiedbyLDAarebasedontheentirecorpus,\n",
      "{ i i }i=1 i i gt\n",
      "are the word embeddings of the rationale, LLM- incontrasttotheaspectswhicharespecifictoindi-\n",
      "generatedsummary,andtheground-truthsummary vidualdocuments. Fromthismodel,wederivethe\n",
      "respectively,thesummaryscoreisaweightedaver- topicdistributionspD ,pA ,andpR forthe\n",
      "LDA i,LDA i,LDA\n",
      "ageoftwosemanticsimilarity: document, thei-thaspects, andthei-thrationale,\n",
      "respectively. Thecoherencescore C iscalculated\n",
      "S = sim Sˆ,Sˆ +ϕ sim Sˆ,Rˆ , (2) ∇i\n",
      "∇i ⟨ i gt ⟩ α · ⟨ i i ⟩ astheKL-divergencebetweenthesedistributions:\n",
      "where ϕ is a hyper-parameter balancing the im-\n",
      "α C = KL(pD pA )\n",
      "portance of two components, and sim is the i LDA i,LDA\n",
      "∇ ||\n",
      "⟨·⟩\n",
      "semantic similarity computation. For example, (1+ϕ ) KL(pD pR ) (3)\n",
      "β LDA i,LDA\n",
      "− · ||\n",
      "sim x,y canbecomputedusingcosinesimilarity\n",
      "as s⟨ im x⟩ ,y = x·y . The first term in Eq. (2) whereϕ β isaparameterthatmanagestheweight\n",
      "empha⟨ sizes⟩ the s|| ix m||· i| l|y a| r| ity between the generated oftheKL(pD LDA||pR i,LDA)termitself,andKL( ·||·)\n",
      "summaryandtheground-truthsummary,whilethe symbolizestheKL-divergencecomputation:\n",
      "second term focus on the relevance between the The score ∇C i in Eq. (3) fosters two primary ob-\n",
      "generatedsummaryandtheprependedrationale,in jectives: (1) −ϕ β · KL(pD LDA ||pR i,LDA), an term\n",
      "avoidscoringhighforlazygenerationbytheLLM that enhances the topical coherence between the\n",
      "(i.e.,simplyrepeatthegivenground-truthsummary documentandrationale. (2)KL(pD LDA||pA i,LDA)\n",
      "−\n",
      "regardlessofthegeneratedrationale). KL(pD pR ),atermwhichencouragesthe\n",
      "LDA|| i,LDA\n",
      "triples (T R ) to refine this coherence beyond\n",
      "i i\n",
      "CoherenceScore. Wealsowanttoevaluatehow ∈\n",
      "whatisachievedbyaspectsalone.\n",
      "theaspectsandrationalealignwiththelatenttopics\n",
      "Thefinalselectionofoptimalrationales,denoted\n",
      "ofthedocument. Here,weemployaLatentDirich-\n",
      "asR = (A ,T ),isbasedonthosethatyieldthe\n",
      "∗ ∗ ∗\n",
      "letAllocation(LDA)model(Bleietal.,2003),an\n",
      "highestcombinedscoreofEq. (2)andEq. (3),and\n",
      "algorithmthatrepresentseachdocumentasablend\n",
      "givenbyEq.(4),\n",
      "of a certain number of topics. To be specific, we\n",
      "representeachdocumentasadistributionoverthe R = argmax ( S +λ C), (4)\n",
      "∗ i i cs i\n",
      "entire lexicon. Given a document D, a rationale ∇ ·∇\n",
      "R ,andaspectsA R ,weinitiallytrainanLDA whereλ isabalancinghyperparameterthatman-\n",
      "i i i cs\n",
      "∈\n",
      "modelonthecorpus(alldocumentsinthedataset) ages the relative contributions of the two scores.\n",
      "toidentifylatenttopicswithourspecifiednumber Wethenusethegoldrationalesasthesupervisiontotrainourlocallightweightlanguagemodelinthe thebestrationaleR ,alongwiththedocumentD,\n",
      "∗\n",
      "followingstep. asthesupervisorysignalforeachtask. Themodel\n",
      "istrainedtominimizetheloss:\n",
      "3.4 Step3: CurriculumLearning\n",
      "(cid:20)\n",
      "(cid:88)\n",
      "TotrainthestudentSeq2Seqlanguagemodelwith = logp(A D;θ )\n",
      "concurrent−early ∗ c\n",
      "L − |\n",
      "theselectedgoldenrationalesforrationalizedtext D∈D\n",
      "(cid:21)\n",
      "summarization,weintroduceanapproachreminis-\n",
      "+logp(T D,A ;θ )+logp(S D,R ;θ ) .\n",
      "cent of curriculum learning (Bengio et al., 2009; ∗ | ∗ c gt | ∗ c\n",
      "Hacohen and Weinshall, 2019; Nagatsuka et al.,\n",
      "UsingtheLLM’soutputasaformofteacherforc-\n",
      "2021; Xu et al., 2020), which facilitates learning\n",
      "ing(Bengioetal.,2015)allowsthemodeltofocus\n",
      "in stages of increasing complexity. This strategy\n",
      "onlearningthestructured(aspect-triple-summary)\n",
      "consistsofthefollowingphases: (1)Singular-task\n",
      "summarizationintheearlystage,withoutitsown\n",
      "learning, (2) Concurrent learning, and (3) Joint\n",
      "flawedpredictiondistractingit.\n",
      "learning. For the first two phases, we focus on\n",
      "LateStage: Self-guidedTraining. Aswetransition\n",
      "the tasks of aspect extraction, triple extraction,\n",
      "tothelaterstages,ourfocuspivotstotrainingthe\n",
      "and summary generation, distinguished by pre-\n",
      "modelusingitsownpredictionsasinputsforsubse-\n",
      "fixtokens AspExt , TriExt ,and SumGen ,\n",
      "⟨ ⟩ ⟨ ⟩ ⟨ ⟩ quenttasks. Thisstrategyischaracterizedbyacas-\n",
      "respectively. We use prefix tokens article ,\n",
      "⟨ ⟩ cading training approach: the model begins with\n",
      "aspects , triples , summary tospecify\n",
      "⟨ ⟩ ⟨ ⟩ ⟨ ⟩ aspect extraction, progresses to triple extraction,\n",
      "D,A,T,andS,respectively.\n",
      "andultimatelyleadstosummarygeneration. The\n",
      "Singular-task learning Initially, we train the benefitofthisapproachstemsfromitssequential\n",
      "model on each task separately, aiding the model information flow, where the outcome of one task\n",
      "in developing a baseline understanding and abil- informs the next. However, a challenge emerges\n",
      "itytohandleeachtaskindividually. Forinstance, duetothecomputationaloverheadofdecodingin-\n",
      "inaspectextraction,weaimtotrainamodelthat termediateresults,suchasaspectsandtriples. To\n",
      "minimizestheloss giventhedocumentD: mitigatethis,whilemaintainingthesequentialin-\n",
      "A\n",
      "L\n",
      "tegrity,weemploygreedydecoding. Thismethod\n",
      "(cid:88)\n",
      "A = logp(A ∗ D;θ s), acceleratestheprocessbyselectingthemostlikely\n",
      "L − |\n",
      "D∈D token at each step, eliminating the need for full-\n",
      "blowngenerationateveryjuncture. Basedonthis,\n",
      "where is the training set of documents,\n",
      "p(A D)D = (cid:81)m p(a D,A<j),withmthelength thelossbecomes:\n",
      "| j=1 j | (cid:20)\n",
      "oftheaspectsintherationale,a thej-thtokenof (cid:88)\n",
      "j = logp(A D;θ )\n",
      "theaspects,andA<j thepreviousgeneratedaspect Lconcurrent−late − ∗ | c\n",
      "D∈D\n",
      "tokens. Themodelfollowsasimilarprocedurefor (cid:21)\n",
      "tripleextractionandsummarygeneration,focusing +logp(T D,A˜;θ )+logp(S D,A˜,T˜;θ ) ,\n",
      "∗ c gt c\n",
      "| |\n",
      "onminimizinglosses and ,respectively:\n",
      "T S\n",
      "L L\n",
      "(cid:88) whereA˜andT˜ representtheintermediateaspects\n",
      "= logp(T D,A ;θ ),\n",
      "T ∗ ∗ s andtriplesobtainedgeneratedthroughgreedyde-\n",
      "L − |\n",
      "D∈D codingbythemodelitself. Theprimaryaimofthis\n",
      "(cid:88)\n",
      "S = logp(S gt D,A ∗,T ∗;θ s). phaseistwofold: (1)todiminishthemodel’sdepen-\n",
      "L − |\n",
      "D∈D dencyonLLM-providedrationalesand,(2)toaug-\n",
      "mentthemodel’scapabilityforautonomouslearn-\n",
      "Concurrent Learning Once the model has be-\n",
      "ing,withtheoverarchingaspirationofenablingit\n",
      "comeproficientinperformingindividualtasks,we\n",
      "togenerateitsownrationalesandsummaries.\n",
      "advancetotheconcurrentlearningphasewherethe\n",
      "modelsimultaneouslylearnsthetasks. Thisphase Joint Learning In the final phase, we enhance\n",
      "allowsfortaskinterplayandreciprocalreinforce- the model’s ability to concurrently generate both\n",
      "mentoflearning. Tofacilitateasmoothtransition, therationaleandthesummaryfromagivendocu-\n",
      "wefurthersplitthisphaseintoearlyandlatestages. mentwiththerationale-summarygenerationtask.\n",
      "Early Stage: LLM-guided Training. In the early Different from the late stage of concurrent learn-\n",
      "phase, weuse the aspectsA and triplesT from ing,thisstagestreamlinestheprocessbycollapsing\n",
      "∗ ∗#Samples #Words n = 15,8,8 times for CNNDM, XSum, and\n",
      "Dataset Train Valid Test Doc. Sum. { }\n",
      "ClinicalTrialrespectively. Thisgeneratesadiverse\n",
      "CNN/DailyMail 287,113 13,368 11,490 766.6 54.8\n",
      "setofpotentialrationalecandidates. Theparame-\n",
      "XSum 204,045 11,332 11,334 414.5 23.0\n",
      "ClinicalTrial 163,088 20,386 20,386 181.4 45.2 tersforthegoldenrationaleselectionaresetasfol-\n",
      "Table1: Statisticsofdatasets. lows: ϕ α = 0.6,ϕ β = 1.3,andλ cs = 1.5. Weuse\n",
      "cosine similarity to calculate the summary score\n",
      "threepairsofencode-decodeprocessesintoasin-\n",
      "with the embeddings retrieved from text-davinci-\n",
      "gle pair. We use the optimal rationale from the\n",
      "003 (a GPT-3.5 model that provides embedding).\n",
      "LLMandtheground-truthsummaryasthelabels.\n",
      "LDA latent topics are specified at 200, 500, and\n",
      "We introduce the prefix token RatGen for this\n",
      "300forCNNDM,XSum,andClinicalTrialrespec-\n",
      "⟨ ⟩\n",
      "task. The model aims to minimize the following\n",
      "tively. Forthejointlearningphase,theparameters\n",
      "lossfunction:\n",
      "arefixedatλ = 0.8andλ = 1.2.\n",
      "R S\n",
      "(cid:20)\n",
      "(cid:88)\n",
      "joint = λ Rlogp(R ∗ D;θ r) Training ForbothCNNDMandXSumdatasets,\n",
      "L − |\n",
      "D∈D we utilize the BART-Large (Lewis et al., 2019)\n",
      "(cid:21)\n",
      "+λ logp(S D,R˜;θ ) , checkpointsthathavebeenfine-tunedspecifically\n",
      "S gt r\n",
      "| for these datasets, as the backbone models. In\n",
      "the case of ClinicalTrial, we fine-tune the BART-\n",
      "where S is the human-annotated ground-truth\n",
      "gt Large CNNDM checkpoint using only the sum-\n",
      "summaryinthedataset,R˜isthegeneratedrationale\n",
      "marytocreateabackbonemodel. Allmodels,in-\n",
      "viagreedydecoding,andλ andλ arehyperpa-\n",
      "R S cludingthebaselines,undergofine-tuningforthree\n",
      "rametersthatbalancetheimportanceofrationale\n",
      "epochs,withanearlystoppingmechanisminplace\n",
      "andsummarygenerations.\n",
      "tooptimizeperformance. Wetrainmodelswithan\n",
      "Throughourstrategicallydesignedcurriculum\n",
      "NVIDIARTXA6000GPU.\n",
      "learning process, the model progressively gains\n",
      "the capability to generate accurate and succinct\n",
      "Baselines WecompareTriSumtobaselineab-\n",
      "rationalesandsummaries.\n",
      "stractivesummarization models includingBERT-\n",
      "SumAbs (Liu, 2019), T5 (Raffel et al., 2020),\n",
      "4 Experiments\n",
      "BART (Lewis et al., 2019), PEGASUS (Zhang\n",
      "DataSource OurevaluationofTriSum iscar- etal.,2020),GSum(Douetal.,2021),BigBird(Za-\n",
      "ried out using three datasets: CNN/Daily Mail heer et al., 2021), SimCLS (Liu and Liu, 2021),\n",
      "(CNNDM) v3.0.0 (Nallapati et al., 2016), XSum SeqCo (Xu et al., 2022), GLM (Du et al., 2022),\n",
      "(Narayan et al., 1808), and a bespoke dataset we andGPT-3.5.\n",
      "havedevelopedfromClinicalTrial1. Thecompre-\n",
      "hensive statistics of these datasets can be found Evaluation We use the following metrics: (1)\n",
      "inTable1. ToconstructtheClinicalTrialdataset, ROUGE-F1: measurestheoverlapofn-gramsbe-\n",
      "we treat the \"detailed description\" from Clinical tween the generated summary and the reference\n",
      "Trialasthedocumentandthe\"briefsummary\"as summary. WemeasureROUGE-1(R-1),ROUGE-\n",
      "itscorrespondingground-truthsummary. Froman 2 (R-2), and ROUGE-L (R-L). (2) BERTScore\n",
      "originaltotalof305,591samples,wehaveselected and BARTScore: measure the semantic similar-\n",
      "203,860 (with a splitting ratio of 8:1:1), filtering itybetweenthegeneratedsummaryandtherefer-\n",
      "outentrieswheredocumentsexceed1,024tokens encesummaryusingpre-trainedlanguagemodels\n",
      "orwheresummariessurpass256tokens. RoBERTa Large andBART Large,respectively.\n",
      "Model and Parameters For the rationale gen-\n",
      "4.1 PerformanceAnalysis\n",
      "eration and the summarization process, we em-\n",
      "ployGPT-3.5(specifically,thegpt-3.5-turbo2)as Tables2and3provideanin-depthlookathowour\n",
      "TriSumapproachperformscomparedtovarious\n",
      "the LLM. In the LLM rationale probing phase,\n",
      "baselinemodels. TheresultsincludebothROUGE\n",
      "we prompt the LLM differently for each dataset:\n",
      "scoresandsemanticsimilaritymetricsacrossdif-\n",
      "1https://clinicaltrials.gov/\n",
      "ferentdatasets,fromgeneralnewssourcestospe-\n",
      "2Weusethecheckpointgpt-3.5-turbo-0613,availableat\n",
      "cializeddomain-specificcollections. Ouranalysis\n",
      "https://platform.openai.com/docs/models/\n",
      "gpt-3-5 revealsseveralkeyinsights:CNN/DailyMail XSum ClinicalTrial\n",
      "Model R-1 R-2 R-L ∆ R-1 R-2 R-L ∆ R-1 R-2 R-L ∆\n",
      "Baselines\n",
      "BERTSumAbs(LiuandLapata,2019) 41.2 18.7 37.2 +13.6% 38.8 16.5 31.0 +28.3% 39.2 19.3 29.6 +19.3%\n",
      "T5Large(Raffeletal.,2020) 42.4 20.8 39.9 +7.0% 40.1 17.2 32.3 +23.5% 41.3 22.1 32.5 +9.6%\n",
      "BARTLarge(Lewisetal.,2019) 44.0 21.1 40.6 +4.4% 45.4 22.3 37.3 +5.4% 43.5 23.3 33.7 +4.6%\n",
      "PEGASUS(Zhangetal.,2020) 44.2 21.6 41.3 +3.0% 46.7 24.4 38.9 +0.6% 41.8 22.9 31.7 +9.0%\n",
      "GSum(Douetal.,2021) 45.5 22.3 42.1 +0.4% 45.1 21.5 36.6 +7.3% 43.5 23.1 32.8 +5.7%\n",
      "BigBird (Zaheeretal.,2021) 43.8 21.1 40.7 +4.5% 47.1 24.1 38.8 +0.6% 44.2 23.8 34.5 +2.5%\n",
      "Large\n",
      "SimCLS(LiuandLiu,2021) 45.6 21.9 41.0 +1.7% 46.6 24.2 39.1 +0.7% 43.8 23.3 34.1 +3.9%\n",
      "SeqCo(Xuetal.,2022) 45.0 21.8 41.8 +1.6% 45.6 22.4 37.0 +5.4% 42.8 22.5 33.2 +6.7%\n",
      "GLMRoBERTa(Duetal.,2022) 43.8 21.0 40.5 +4.7% 45.5 23.5 37.3 +4.1% 43.3 23.0 33.9 +4.9%\n",
      "GPT-3.5zero-shot 37.4 13.8 29.1 +37.4% 26.6 6.7 18.8 +112.5% 34.8 12.8 23.5 +47.8%\n",
      "OurMethod\n",
      "GPT-3.5w/TriSumrationale 46.7 23.5 40.7 −0.5% 34.4 12.6 28.4 +46.8% 44.6 24.5 30.4 +5.6%\n",
      "TriSum-S 45.9 22.8 42.3 −0.6% 47.4 24.8 39.4 −1.0% 45.3 24.8 35.0 +0.0%\n",
      "TriSum-C 45.5 22.3 41.2 +1.2% 46.5 24.0 38.7 +1.1% 44.2 23.7 34.4 +2.7%\n",
      "TriSum-J 45.7 22.7 41.9 — 47.3 24.4 39.0 — 45.3 24.6 35.2 —\n",
      "Table 2: Performance comparison of ROUGE Scores across CNN/DailyMail, XSum, and ClinicalTrial\n",
      "datasets. ThelabelsTriSum-S,TriSum-C,andTriSum-Jsignifymodelcheckpointsattheendofsingular-\n",
      "task,concurrent,andjointlearningstages,respectively. ForTriSum-S,distinctoptimalcheckpoints,eachtailored\n",
      "for a specific task, are used in a pipeline of three Seq2Seq models. The symbol ∆ signifies the percentage\n",
      "improvementintheaggregateROUGEscoresachievedbyTriSum-J.Thetop-3resultsarehighlighted. Our\n",
      "backbonemodelBART isshadedforreference.\n",
      "Large\n",
      "CNN/DailyMail XSum ClinicalTrial theeffectivenessofincludingtheLLM-generated\n",
      "Model BS BAS BS BAS BS BAS rationales as the additional supervision and indi-\n",
      "Baselines catesthepotentialofourmethodtobescaledfor\n",
      "BERTSumAbs 85.76 -3.81 87.23 -3.66 85.41 -3.79\n",
      "T5Large 87.22 -3.71 90.73 -2.70 87.76 -2.89 the enhancement of other summarization models\n",
      "BARTLarge 87.98 -3.45 91.62 -2.50 88.30 -2.79\n",
      "aswell. Notably, TriSum-Sconsistentlyexcels\n",
      "PEGASUS 87.37 -3.64 91.90 -2.44 87.62 -2.80\n",
      "GSum 87.83 -3.54 91.23 -2.57 88.41 -2.75 in performance. This heightened effectiveness is\n",
      "BigBird 88.03 -3.38 91.97 -2.40 89.45 -2.67\n",
      "SimCLSLarge 88.28 -3.39 90.78 -2.93 87.85 -3.15 rootedinitsmodulardesign,whichencompasses\n",
      "SeqCo 87.47 -3.56 91.35 -2.56 88.06 -2.93\n",
      "threecheckpoints,eachoptimizedforauniquetask.\n",
      "GLMRoBERTa 87.33 -3.69 91.87 -2.51 88.55 -2.84\n",
      "GPT-3.5zero-shot 87.70 -3.36 87.67 -2.80 87.08 -3.01 Therefore,theimprovedresultsmaybeattributed\n",
      "OurMethod toitsthrice-enlargedparameterset,whencompared\n",
      "GPT-3.5∗ 89.20 -3.14 89.25 -2.58 89.20 -2.55\n",
      "TriSum\n",
      "TriSum-S 88.48 -3.22 91.95 -2.38 90.05 -2.47 toTriSum-CorTriSum-J.\n",
      "TriSum-C 87.21 -3.76 90.88 -2.84 89.40 -2.59\n",
      "TriSum-J 88.50 -3.25 92.17 -2.33 89.97 -2.53\n",
      "Optimized Rationale for LLM Interestingly,\n",
      "Table 3: Pre-trained language model-evaluated se- the rationales generated by TriSum can sig-\n",
      "mantic similarity scores. “*” indicate the inference\n",
      "nificantly improve the performance of GPT-3.5\n",
      "with TriSum-generated rationale. “BS” and “BAS”\n",
      "withinthedataset(+40.9%ROUGEScore,+2.0%\n",
      "areBERTScoreandBARTScore, respectively. Top-3\n",
      "BERTScore,and+9.9%BARTScorecomparedto\n",
      "resultsarehighlighted.\n",
      "GPT-3.5 ). For example, in our tests with\n",
      "zero-shot\n",
      "ConsistentEdgeOverBaselines TheTriSum the CNNDM dataset, the LLM, guided by the\n",
      "approachconsistentlyoutperformsmanystate-of- TriSum’s rationale and without any fine-tuning,\n",
      "the-artmodelsacrossdifferentdatasets,highlight- outperformalltheotherfine-tunedmodelsinterms\n",
      "ing its strength and adaptability. Statistically, in of ROUGE-1 score. This suggests that users can\n",
      "termsofoverallROUGEscores,TriSum-Jout- usefine-tunedTriSumtoguidetheLLMincreat-\n",
      "performs fine-tuned models (excluding GPT-3.5) ingqualitysummaries.\n",
      "by 4.5% on CNNDM, 8.5% on XSum, and 7.4%\n",
      "EffectofCurriculumLearning Figure4shows\n",
      "onClinicalTrial.\n",
      "thebenefitsofcurriculumlearningonthemodel’s\n",
      "Gains Over Backbone We use BART as the task performance. Two key comparisons are ev-\n",
      "backbone model, which is already known for its ident: the raw model versus one trained with\n",
      "performance in summarization tasks. The no- singular-tasklearningintheearlyconcurrentlearn-\n",
      "ticeable overall improvement across all datasets ing stage, and the raw model versus one trained\n",
      "(+4.8% ROUGE score and +1.0% BERTScore, throughtheprevioustwolearningstages. Theabla-\n",
      "and+7.3%BARTScore)whenusingtheTriSum tionstudyfurtherrevealsastep-wiseperformance\n",
      "approach over BART is significant. This shows improvement. Notably,whentrainedsolelyonjointArticle:\n",
      "(CNN) Four workers died in a massive oil rig fire that raged for hours off the coast of Mexico Wednesday. Mexican state oil company Pemex said 45 workers were injuredin the blaze,\n",
      "which began early Wednesday morning. Two of them are in serious condition, the company said. Authorities evacuated about 300 peoplefrom the Abkatun Permanente platform after\n",
      "the fire started, Pemex said. At least 10 boats worked to battle the blaze for hours. The fire had been extinguishedby Wednesday night, Pemex said in a Twitter post. The company denied\n",
      "rumors that the platform had collapsed and said there was no oil spill as a result of the fire. The state oil company hasn't said what caused the fire on the platform, which is located in the\n",
      "Gulf of Mexico's Campeche Sound. The fire began in the platform's dehydration and pumping area, Pemex said. CNN's Mayra Cuevas contributed to this report.\n",
      "Ground truth summary: TriSum rationale:\n",
      "The fire on a platform in the Gulf of Mexico has been extinguished, <rationale> <aspects> Massive oil rig fire off the coast of Mexico. Four workers died. 45 workers\n",
      "Pemex says . 45 workers were injuredin the blaze, according to the injured. Two workers in serious condition. 300 people evacuated. Fire extinguished by Wednesday night.\n",
      "state oil company . Four workers were killed in the oil rig fire, which No oil spill. <triples> [Four workers | were died in | oil rig fire], [45 workers | were injured in | the blaze],\n",
      "started early Wednesday . [Two workers | are in | serious condition], [300 people | were evacuated from | the platform], [The fire |\n",
      "had been extinguished by | Wednesday night], [The fire | did not result in | oil spill].\n",
      "BART summary:\n",
      "About 300 people were evacuatedfrom the Abkatun Permanente TriSum summary:\n",
      "platform. The fire began in the platform's dehydration and pumping Four workers were died in an oil rig fire. 45 workers were injured, two of them in a serious condition, state\n",
      "area, Pemex says. The company denies rumors that the platform had oil company Pemex says. About 300 people were evacuatedafter the fire started early Wednesday. The fire\n",
      "collapsed and said there was no oil spill as a result of the fire. has been extinguished and it did not result in oil spill, the company says.\n",
      "Figure3: AnexampleofabstractivesummarizationonCNN/DailyMaildataset. Wecomparethesummary\n",
      "generatedbyourTriSumapproachtotheground-truthsummaryandtheonegeneratedbyBART.Weusedifferent\n",
      "colorstoshowthedistincttopicsinthearticleandsummary.\n",
      "early late\n",
      "Singular-task Learning Concurrent Learning Joint Learning\n",
      "ssoL\n",
      "noitadilaV\n",
      "ROUGE-1 ROUGE-2 ROUGE-L\n",
      "47 25 43\n",
      "46 24\n",
      "42\n",
      "45 23\n",
      "22 41 44\n",
      "21\n",
      "43 40\n",
      "20\n",
      "501002005001K3K5K 501002005001K3K5K 501002005001K3K5K\n",
      "# LDA Topics # LDA Topics # LDA Topics\n",
      "Figure5: PerformancebydifferentnumbersofLDA\n",
      "phase\n",
      "0 1 2 3 latenttopicsspecifiedingoldenrationaleselection.\n",
      "Singular Concurrent Concurrent Joint R-1 R-2 R-L WecomparetheROUGEscoresofthesummariesgen-\n",
      "-Early -Late\n",
      "✓ ✓ ✓ ✓ 45.7 22.7 41.9 eratedbyTriSum-RonCNN/DailyMaildataset.\n",
      "✓ ✓ ✕ ✓ 45.3 (↓0.4) 22.2 (↓0.5) 41.0 (↓0.9)\n",
      "✓ ✕ ✕ ✓ 44.4 (↓1.3) 21.3 (↓1.4) 40.4 (↓1.5)\n",
      "✕ ✕ ✕ ✓ 42.3 (↓3.4) 20.5 (↓2.2) 38.4 (↓3.5) termathintermsoffatalities,injuries,andcontain-\n",
      "Figure 4: Validation loss by training steps and ab- ment. BART’srendition,whiledetailedaboutthe\n",
      "lation study for curriculum learning on CNN/Dai- evacuationandfire’sorigin,missesoutonpivotal\n",
      "lyMail. AspExt, TriExt, and SumGen denote aspect informationlikethedeathtollandinjuryscale. On\n",
      "extraction, triple extraction, and summary generation theotherhand,TriSum’srationalebeginsbyitem-\n",
      "tasks, respectively. -early/-late denote the early/late\n",
      "izing the essential aspects of the incident. These\n",
      "stageofconcurrentlearning. -rawdenotestrainingthe\n",
      "aspectspresentahigh-leveloverviewoftheevents\n",
      "modelfromscratch.\n",
      "and their aftermath. Following these aspects, the\n",
      "tripleszoomintothespecifics,elucidatingtherela-\n",
      "learningfromscratch,themodelunderperformsthe\n",
      "tionsbetweentheentitiesinvolved. Thistechnique\n",
      "originalBART.Thisemphasizestheindispensable\n",
      "used by TriSum ensures a comprehensive sum-\n",
      "role of foundational tasks, without which BART\n",
      "maryandimprovesclarity. Readerscanfollowthe\n",
      "struggleswiththerationale-summarygeneration.\n",
      "summary’s content back to its main aspects and\n",
      "EffectofGoldenRationaleSelection Figure5 detailedtriples,gainingadeeperunderstandingof\n",
      "demonstrates the impact of our golden rationale howthesummarizationprocessworks. Thistrans-\n",
      "selection. The performance of the trained model parencyisakeyfeatureofTriSum,allowingusers\n",
      "dropssignificantlywhenthenumberoflatenttopics tograspthereasoningbehindthesummarizedcon-\n",
      "iseithertoolow(e.g.,50)orhigh(e.g.,5000). On tent. WeprovidemoreexamplesintheAppendix.\n",
      "the other hand, choosing an appropriate number\n",
      "of topics (e.g., 200) leads to improved outcomes. 5 Conclusion\n",
      "Thisunderscorestheimportanceofthequalityof\n",
      "WeintroducedTriSum,anapproachaimedatdis-\n",
      "rationales; poor-quality rationales can negatively\n",
      "tillingsummarizationcapabilitiesfromalargelan-\n",
      "impact the model, emphasizing the value of our\n",
      "guage model to a small local model. Extensive\n",
      "rationaleselectionstrategy.\n",
      "experimentsverifieditssuperiorperformanceover\n",
      "Case Study Figure 3 compares summaries cre- state-of-the-artmodelsacrossdiversedatasetson\n",
      "atedfromaCNNarticlediscussinganoilrigfire theabstractivesummarizationtask. Ourworkhigh-\n",
      "in Mexico. The ground truth summary adeptly lights the potential of leveraging large model in-\n",
      "encapsulatesthemainevents,emphasizingtheaf- sightsforefficientandnuancedtextsummarization.References Dublin,Ireland.AssociationforComputationalLin-\n",
      "guistics.\n",
      "SiqiBao,HuangHe,FanWang,HuaWu,HaifengWang,\n",
      "WenquanWu,ZhenGuo,ZhibinLiu,andXinchao ZorikGekhman,JonathanHerzig,RoeeAharoni,Chen\n",
      "Xu. 2021. PLATO-2: Towards building an open- Elkind,andIdanSzpektor.2023. Trueteacher:Learn-\n",
      "domain chatbot via curriculum learning. In Find- ing factual consistency evaluation with large lan-\n",
      "ingsoftheAssociationforComputationalLinguis- guagemodels.\n",
      "tics: ACL-IJCNLP2021,pages2513–2525,Online.\n",
      "AssociationforComputationalLinguistics. Guy Hacohen and Daphna Weinshall. 2019. On the\n",
      "power of curriculum learning in training deep net-\n",
      "SamyBengio,OriolVinyals,NavdeepJaitly,andNoam works. InInternationalconferenceonmachinelearn-\n",
      "Shazeer. 2015. Scheduled sampling for sequence ing,pages2535–2544.PMLR.\n",
      "predictionwithrecurrentneuralnetworks. Advances\n",
      "inneuralinformationprocessingsystems,28. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\n",
      "Distillingtheknowledgeinaneuralnetwork. arXiv\n",
      "Yoshua Bengio, Jérôme Louradour, Ronan Collobert, preprintarXiv:1503.02531.\n",
      "and Jason Weston. 2009. Curriculum learning. In\n",
      "Proceedingsofthe26thannualinternationalconfer- NamgyuHo,LauraSchmid,andSe-YoungYun.2023.\n",
      "enceonmachinelearning,pages41–48. Largelanguagemodelsarereasoningteachers.\n",
      "David M Blei, Andrew Y Ng, and Michael I Jordan. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,\n",
      "2003. Latentdirichletallocation. Journalofmachine HootanNakhost,YasuhisaFujii,AlexanderRatner,\n",
      "Learningresearch,3(Jan):993–1022. Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.\n",
      "2023. Distillingstep-by-step! outperforminglarger\n",
      "ThorstenBrants,AshokCPopat,PengXu,FranzJOch, languagemodelswithlesstrainingdataandsmaller\n",
      "andJeffreyDean.2007. Largelanguagemodelsin modelsizes.\n",
      "machinetranslation.\n",
      "XiaoqiJiao,YichunYin,LifengShang,XinJiang,Xiao\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Chen, Linlin Li, Fang Wang, and Qun Liu. 2019.\n",
      "Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind Tinybert: Distillingbertfornaturallanguageunder-\n",
      "Neelakantan,PranavShyam,GirishSastry,Amanda standing. arXivpreprintarXiv:1909.10351.\n",
      "Askell,etal.2020. Languagemodelsarefew-shot\n",
      "learners. Advancesinneuralinformationprocessing Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\n",
      "systems,33:1877–1901. Ghazvininejad,AbdelrahmanMohamed,OmerLevy,\n",
      "VesStoyanov,andLukeZettlemoyer.2019. Bart:De-\n",
      "Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, noisingsequence-to-sequencepre-trainingfornatural\n",
      "and Jingjing Liu. 2019. Distilling knowledge languagegeneration,translation,andcomprehension.\n",
      "learned in bert for text generation. arXiv preprint arXivpreprintarXiv:1910.13461.\n",
      "arXiv:1911.03829.\n",
      "Ying-JiaLin,DanielTan,Tzu-HsuanChou,Hung-Yu\n",
      "AakankshaChowdhery,SharanNarang,JacobDevlin, Kao, and Hsin-Yang Wang. 2020. Knowledge dis-\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts, tillationonextractivesummarization. In2020IEEE\n",
      "Paul Barham, Hyung Won Chung, Charles Sutton, ThirdInternationalConferenceonArtificialIntelli-\n",
      "Sebastian Gehrmann, et al. 2022. Palm: Scaling gence and Knowledge Engineering (AIKE), pages\n",
      "language modeling with pathways. arXiv preprint 71–76.IEEE.\n",
      "arXiv:2204.02311.\n",
      "Yang Liu. 2019. Text summarization with pretrained\n",
      "Finale Doshi-Velez and Been Kim. 2017. Towards a encoders. InProceedingsofthe2019Conferenceon\n",
      "rigorous science of interpretable machine learning. EmpiricalMethodsinNaturalLanguageProcessing\n",
      "arXivpreprintarXiv:1702.08608. andthe9thInternationalJointConferenceonNatu-\n",
      "ralLanguageProcessing(EMNLP-IJCNLP),pages\n",
      "Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao\n",
      "3730–3740,HongKong,China.AssociationforCom-\n",
      "Jiang, and Graham Neubig. 2021. GSum: A gen-\n",
      "putationalLinguistics.\n",
      "eralframeworkforguidedneuralabstractivesumma-\n",
      "rization. InProceedingsofthe2021Conferenceof Yang Liu and Mirella Lapata. 2019. Text summa-\n",
      "theNorthAmericanChapteroftheAssociationfor rization with pretrained encoders. arXiv preprint\n",
      "ComputationalLinguistics: HumanLanguageTech- arXiv:1908.08345.\n",
      "nologies,pages4830–4842,Online.Associationfor\n",
      "ComputationalLinguistics. YixinLiu,AlexanderRFabbri,PengfeiLiu,Dragomir\n",
      "Radev, and Arman Cohan. 2023. On learning to\n",
      "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, summarizewithlargelanguagemodelsasreferences.\n",
      "JiezhongQiu,ZhilinYang,andJieTang.2022. GLM: arXivpreprintarXiv:2305.14239.\n",
      "Generallanguagemodelpretrainingwithautoregres-\n",
      "siveblankinfilling. InProceedingsofthe60thAn- Yixin Liu and Pengfei Liu. 2021. SimCLS: A sim-\n",
      "nualMeetingoftheAssociationforComputational pleframeworkforcontrastivelearningofabstractive\n",
      "Linguistics(Volume1:LongPapers),pages320–335, summarization. InProceedingsofthe59thAnnualMeeting of the Association for Computational Lin- SamShleiferandAlexanderM.Rush.2020. Pre-trained\n",
      "guisticsandthe11thInternationalJointConference summarizationdistillation.\n",
      "onNaturalLanguageProcessing(Volume2: Short\n",
      "Papers),pages1065–1072,Online.Associationfor Kumar Shridhar, Alessandro Stolfo, and Mrinmaya\n",
      "ComputationalLinguistics. Sachan.2023. Distillingreasoningcapabilitiesinto\n",
      "smallerlanguagemodels.\n",
      "YixinLiu,PengfeiLiu,DragomirRadev,andGraham\n",
      "Neubig.2022. BRIO:Bringingordertoabstractive EmmaStrubell,AnanyaGanesh,andAndrewMcCal-\n",
      "summarization. InProceedingsofthe60thAnnual lum. 2019. Energy and policy considerations for\n",
      "Meeting of the Association for Computational Lin- deep learning in NLP. In Proceedings of the 57th\n",
      "guistics(Volume1: LongPapers),pages2890–2903, AnnualMeetingoftheAssociationforComputational\n",
      "Dublin,Ireland.AssociationforComputationalLin-\n",
      "Linguistics,pages3645–3650,Florence,Italy.Asso-\n",
      "guistics. ciationforComputationalLinguistics.\n",
      "Lucie Charlotte Magister, Jonathan Mallinson, Jakub Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\n",
      "Adamek, Eric Malmi, and Aliaksei Severyn. 2023. Vechtomova,andJimmyLin.2019. Distillingtask-\n",
      "Teachingsmalllanguagemodelstoreason. specificknowledgefrombertintosimpleneuralnet-\n",
      "works. arXivpreprintarXiv:1903.12136.\n",
      "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\n",
      "Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, HugoTouvron,ThibautLavril,GautierIzacard,Xavier\n",
      "Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Martinet,Marie-AnneLachaux,TimothéeLacroix,\n",
      "Factscore: Fine-grainedatomicevaluationoffactual Baptiste Rozière, Naman Goyal, Eric Hambro,\n",
      "precisioninlongformtextgeneration. Faisal Azhar, et al. 2023. Llama: Open and effi-\n",
      "cient foundation language models. arXiv preprint\n",
      "Koichi Nagatsuka, Clifford Broni-Bediako, and arXiv:2302.13971.\n",
      "MasayasuAtsumi.2021. Pre-trainingaBERTwith\n",
      "curriculumlearningbyincreasingblock-sizeofinput Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "text. InProceedingsoftheInternationalConference Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\n",
      "onRecentAdvancesinNaturalLanguageProcess- Kaiser, and Illia Polosukhin. 2017a. Attention is\n",
      "ing (RANLP 2021), pages 989–996, Held Online. all you need. In Advances in Neural Information\n",
      "INCOMALtd.\n",
      "ProcessingSystems,volume30.CurranAssociates,\n",
      "Inc.\n",
      "RameshNallapati,BowenZhou,CaglarGulcehre,Bing\n",
      "Xiang,etal.2016. Abstractivetextsummarization Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "usingsequence-to-sequencernnsandbeyond. arXiv Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n",
      "preprintarXiv:1602.06023. Kaiser,andIlliaPolosukhin.2017b. Attentionisall\n",
      "youneed. Advancesinneuralinformationprocessing\n",
      "Shashi Narayan, Shay B Cohen, and Mirella Lapata. systems,30.\n",
      "1808. Don’tgivemethedetails,justthesummary!\n",
      "Topic-AwareConvolutionalNeuralNetworksforEx- PeifengWang,AaronChan,FilipIlievski,MuhaoChen,\n",
      "tremeSummarization.ArXiv,abs. andXiangRen.2022. Pinto: Faithfullanguagerea-\n",
      "soning using prompt-generated rationales. arXiv\n",
      "OpenAI.2023. Gpt-4technicalreport. preprintarXiv:2211.01562.\n",
      "DragomirR.Radev,EduardHovy,andKathleenMcKe- ShuohangWang, YangLiu, YichongXu, Chenguang\n",
      "own.2002. Introductiontothespecialissueonsum- Zhu, and Michael Zeng. 2021. Want to reduce la-\n",
      "marization. ComputationalLinguistics,28(4):399– beling cost? GPT-3 can help. In Findings of the\n",
      "408. AssociationforComputationalLinguistics: EMNLP\n",
      "2021,pages4195–4205,PuntaCana,DominicanRe-\n",
      "ColinRaffel,NoamShazeer,AdamRoberts,Katherine public.AssociationforComputationalLinguistics.\n",
      "Lee,SharanNarang,MichaelMatena,YanqiZhou,\n",
      "WeiLi,andPeterJLiu.2020. Exploringthelimits ZifengWang,ChufanGao,CaoXiao,andJimengSun.\n",
      "oftransferlearningwithaunifiedtext-to-texttrans- 2023. AnyPredict: Foundation model for tabular\n",
      "former. TheJournalofMachineLearningResearch, prediction. arXivpreprintarXiv:2305.12081.\n",
      "21(1):5485–5551.\n",
      "JasonWei,XuezhiWang,DaleSchuurmans,Maarten\n",
      "Marco Tulio Ribeiro, Sameer Singh, and Carlos Bosma,BrianIchter,FeiXia,EdChi,QuocLe,and\n",
      "Guestrin.2016. \"whyshoulditrustyou?\"explaining DennyZhou.2023. Chain-of-thoughtpromptingelic-\n",
      "thepredictionsofanyclassifier. InProceedingsof itsreasoninginlargelanguagemodels.\n",
      "the22ndACMSIGKDDinternationalconferenceon\n",
      "knowledgediscoveryanddatamining,pages1135– Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan\n",
      "1144. Wang, Hongtao Xie, and Yongdong Zhang. 2020.\n",
      "Curriculumlearningfornaturallanguageunderstand-\n",
      "Victor Sanh, Lysandre Debut, Julien Chaumond, and ing. InProceedingsofthe58thAnnualMeetingof\n",
      "Thomas Wolf. 2019. Distilbert, a distilled version theAssociationforComputationalLinguistics,pages\n",
      "of bert: smaller, faster, cheaper and lighter. arXiv 6095–6104,Online.AssociationforComputational\n",
      "preprintarXiv:1910.01108. Linguistics.ShushengXu,XingxingZhang,YiWu,andFuruWei.\n",
      "2022. Sequence level contrastive learning for text\n",
      "summarization.\n",
      "WeiYang,YuqingXie,AileenLin,XingyuLi,Luchen\n",
      "Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\n",
      "End-to-end open-domain question answering with\n",
      "BERTserini. In Proceedings of the 2019 Confer-\n",
      "enceoftheNorthAmericanChapteroftheAssocia-\n",
      "tionforComputationalLinguistics(Demonstrations),\n",
      "pages72–77,Minneapolis,Minnesota.Association\n",
      "forComputationalLinguistics.\n",
      "XianjunYang,YanLi,XinluZhang,HaifengChen,and\n",
      "WeiCheng.2023. Exploringthelimitsofchatgptfor\n",
      "queryoraspect-basedtextsummarization.\n",
      "Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi\n",
      "Feng. 2020. Reclor: A reading comprehension\n",
      "datasetrequiringlogicalreasoning. arXivpreprint\n",
      "arXiv:2002.04326.\n",
      "Manzil Zaheer, Guru Guruganesh, Avinava Dubey,\n",
      "Joshua Ainslie, Chris Alberti, Santiago Ontanon,\n",
      "PhilipPham,AnirudhRavula,QifanWang,LiYang,\n",
      "andAmrAhmed.2021. Bigbird: Transformersfor\n",
      "longersequences.\n",
      "Omar Zaidan and Jason Eisner. 2008. Modeling an-\n",
      "notators: A generative approach to learning from\n",
      "annotatorrationales. InProceedingsofthe2008con-\n",
      "ferenceonEmpiricalmethodsinnaturallanguage\n",
      "processing,pages31–40.\n",
      "JingqingZhang,YaoZhao,MohammadSaleh,andPe-\n",
      "terLiu.2020. Pegasus: Pre-trainingwithextracted\n",
      "gap-sentencesforabstractivesummarization. InIn-\n",
      "ternationalConferenceonMachineLearning,pages\n",
      "11328–11339.PMLR.\n",
      "ShengqiangZhang,XingxingZhang,HangboBao,and\n",
      "Furu Wei. 2022. Attention temperature matters in\n",
      "abstractivesummarizationdistillation. InProceed-\n",
      "ingsofthe60thAnnualMeetingoftheAssociation\n",
      "forComputationalLinguistics(Volume1: LongPa-\n",
      "pers),pages127–141,Dublin,Ireland.Association\n",
      "forComputationalLinguistics.A Ethics,Limitations,andRisks\n",
      "Given a document and its ground-truth summary, do\n",
      "the following tasks:\n",
      "A.1 Ethics\n",
      "(1) According to the ground-truth summary, extract\n",
      "essential aspects of the document.\n",
      "Data Privacy and Source: All datasets used in (2) For each essential aspect, retrieve detailed\n",
      "triples in the format [ENTITY1 | RELATION |\n",
      "thisresearch,namelyCNN/DailyMail,XSum,and\n",
      "ENTITY2] used to compose the ground-truth summary.\n",
      "ClinicalTrial,arepubliclyavailable345. Thistrans- (3) With the retrieved triples, compose a summary.\n",
      "parencyminimizesethicalconcernsrelatedtodata The essential aspects, triples, and composed\n",
      "summary should be in the same response, separated\n",
      "sourcingandusage. by a new line.\n",
      "Interpretability: The transparency and inter-\n",
      "All triples [ENTITY1 | RELATION | ENTITY2] should\n",
      "pretability of AI models are ethical imperatives be in length 3 (separated by \"|\").\n",
      "inmanyapplications. TriSumnotonlyimproves Example:\n",
      "================Example=================\n",
      "summarizationperformancebutalsoenhancesthe Prompt:\n",
      "[Document]: [document]\n",
      "interpretabilityofthesummarizationprocess,mak-\n",
      "[Ground-truth Summary]: [ground-truth summary]\n",
      "ingitmoretrustworthy.\n",
      "Update:\n",
      "Essential Aspects:\n",
      "A.2 Limitations [aspects]\n",
      "Triples:\n",
      "DependenceonLLMs: TriSum’seffectiveness\n",
      "- [ENTITY1_1 | RELATION_1 | ENTITY1_2]\n",
      "iscontingentonthequalityandcapabilitiesofthe - [ENTITY2_1 | RELATION_2 | ENTITY2_2]\n",
      "- [ENTITY3_1 | RELATION_3 | ENTITY3_2]\n",
      "LLMs it distills from. If the LLM has biases or - ...\n",
      "inaccuracies,thesecouldpotentiallybetransferred Generated Summary:\n",
      "[summary]\n",
      "tothelocalmodel.\n",
      "========================================\n",
      "ScopeofRationales: Theaspect-triplerationales,\n",
      "Prompt:\n",
      "whileenhancinginterpretability,mightnotcapture [Document]: {doc}\n",
      "[Ground-truth Summary]: {gt_summary}\n",
      "allnuancesoftheoriginaltext. Someinformation\n",
      "Update:\n",
      "mightbelostoroversimplifiedduringthedistilla-\n",
      "tionprocess.\n",
      "Figure 6: Template used for prompting rationale and\n",
      "summaryfromLLM\n",
      "A.3 Risks\n",
      "Overfitting: There’s a potential risk that the lo-\n",
      "calmodelmightoverfittotherationalesandsum- Given a document, summarize the document in\n",
      "mariesderivedfromtheLLM,leadingtoreduced one sentence: for XSum\n",
      "generalizationonunseendata. Given a document, summarize the document in\n",
      "three sentence: for CNNDM & ClinicalTrial\n",
      "Misinterpretation: Enhancedinterpretabilitycan\n",
      "sometimes lead users to place undue trust in the Document: {doc}\n",
      "model’s outputs. Users should be cautious and Summary:\n",
      "considerthemodel’soutputsasoneofmanytools\n",
      "indecision-makingprocesses. Figure7: Templateusedforpromptingsummaryfrom\n",
      "Ethical Misuse: Like all summarization tools, LLMinzero-shotsetting.\n",
      "there’sariskthatusersmightmisuseTriSum to\n",
      "misrepresentcomplexinformation,leadingtomis- to (1) generate essential aspects of the document\n",
      "information. withrespecttotheground-truthsummary;(2)ex-\n",
      "tract triples from the document that elaborate on\n",
      "B TemplatesUsedforPromptingLLM\n",
      "thesekeyaspects;(3)generateasummaryreferring\n",
      "to both the retrieved triples and the ground-truth\n",
      "Inthissection,weshowcasethetemplatesweused\n",
      "summary. ThetemplatetheninstructstheLLMto\n",
      "forpromptingthelargelanguagemodelfordiffer-\n",
      "generateinaspecificformat,toreducetherandom-\n",
      "entpurposes.\n",
      "nessoftheLLM’soutput. Thedocumentandthe\n",
      "Figure 6 shows the template we use for Step 1\n",
      "ground-truthsummaryareinputtotheplaceholders\n",
      "(LLM Rationale Probing). It instructs the LLM\n",
      "tofinalizethepromptingrequest.\n",
      "3https://github.com/abisee/\n",
      "Figures7and8showthetemplatesweusefortest-\n",
      "cnn-dailymail\n",
      "4https://github.com/EdinburghNLP/XSum ingtheLLM’ssummarizationabilityinazero-shot\n",
      "5https://clinicaltrials.gov/ settingandwithTriSum-generatedrationales,re-• Usage: Duetoitssubstantialsizeandreal-world\n",
      "Given a document and the rationale for\n",
      "data,CNN/DailyMailhasbeenabenchmarkfor\n",
      "summarization, summarize the document in one\n",
      "sentence. several state-of-the-art summarization models,\n",
      "The rationale contains (1) the essential enabling researchers to compare performances\n",
      "aspects of the document; (2) triples of andstrategiesacrossdiversemethods.\n",
      "entities and relations in the document that\n",
      "compose the summary, in the format of\n",
      "[ENTITY1 | RELATION | ENTITY2].\n",
      "XSum XSum (Extreme Summarization) dataset\n",
      "We use the prefixs <aspects> and <triples> to\n",
      "indicate the start of the rationale for providesamorechallengingscenarioforabstrac-\n",
      "aspects and triples, respectively.\n",
      "tivesummarization. Theoverviewofthisdatasetis\n",
      "The generated summary should not longer than describedasfollows:\n",
      "one sentence. for XSum\n",
      "The generated summary should not longer than\n",
      "• Size: It contains 204,045 training examples,\n",
      "three sentence. for CNNDM & ClinicalTrial\n",
      "11,332validationexamples,and11,334testex-\n",
      "Example:\n",
      "================Example================= amples,whicharethearticlescollectedfromthe\n",
      "Prompt: BBC(BritishBroadcastingCorporation).\n",
      "[Document]: [document]\n",
      "[Rationale]: <aspects> + [aspects] +\n",
      "<triples> + [triples]\n",
      "• Content: Unlike CNN/DailyMail where sum-\n",
      "Update: mariesareconstructedfromhighlights,eacharti-\n",
      "Summary:\n",
      "[summary] cleintheXSumdatasetispairedwithasingle-\n",
      "sentencesummary,oftenwritteninastylethatis\n",
      "========================================\n",
      "notpresentinthearticlebody.\n",
      "Prompt:\n",
      "[Document]: {doc}\n",
      "[Rationale]: {aspects} {triples} • NatureofSummaries: ThesummariesinXSum\n",
      "Update: are more abstractive in nature and are not sim-\n",
      "plyextractivesnippetsfromthearticles. Thisde-\n",
      "Figure 8: Template used for prompting sum- mandsmodelstotrulyunderstandthecontentand\n",
      "mary from LLM given TriSum-generated rationale generateauniquesummarizingsentence,making\n",
      "(GPT-3.5 TriSum). it a challenging dataset for abstractive summa-\n",
      "rization.\n",
      "spectively.\n",
      "• Usage: XSum’s distinctive nature has made it\n",
      "C DatasetDescription\n",
      "a preferred choice for researchers focusing on\n",
      "CNN/DailyMail The CNN/DailyMail dataset is advancedabstractivemethodsinsummarization.\n",
      "oneofthemostpopulardatasetsforextractiveand Itssummaries, beingcreativelycraftedandnot\n",
      "abstractivesummarizationtasks. Originatingfrom directlyextractedfromthetext,testthegenuine\n",
      "onlinenewsstories,thedatasetcomprisesarticles abstractingcapabilitiesofmodels.\n",
      "fromCNNandDailyMailwebsites. Theoverview\n",
      "ofthisdatasetisdescribedasfollows: ClinicalTrialWecollectedtheclinicaltrialproto-\n",
      "coldocumentsfromclinicaltrials.govwherethere\n",
      "• Size: It contains 287,113 training examples,\n",
      "areover400Kregisteredclinicaltrialsacrossthe\n",
      "13,368validationexamples,and11,490testex-\n",
      "world. Theoverviewofthisdatasetisdescribedas\n",
      "amples.\n",
      "follows:\n",
      "• Content: Each example in the dataset consists\n",
      "• Size: Wedownloadedthestaticcopyofthewhole\n",
      "ofanewsarticleandseveralaccompanyinghigh-\n",
      "clinicaltrialdatabasewhichiswitharound460K\n",
      "lightpoints,which,whencombined,formaco-\n",
      "clinicaltrialdocuments. 203,860wereselected\n",
      "herentsummaryofthemainarticle.\n",
      "outofallbasedonthestandard(a)theyareinter-\n",
      "• NatureofSummaries: Thehighlights,crafted ventionalclinicaltrials,(b)missingorduplicate\n",
      "to engage a reader’s attention, effectively form titles,(c)missingthebriefsummarysection. To\n",
      "summaries. Typically,asummaryconsistsof2 fitthecontextwindowofusedlanguagemodels,\n",
      "to 3 sentences. They can be approached either we further exclude documents that have more\n",
      "extractively or abstractively by summarization than 1024 tokens or the target summaries are\n",
      "models. withmorethan256tokens.• Content: Theclinicaltrialdocumentdescribes\n",
      "theproposalfortestingtheeffectivenessandthe\n",
      "safety of a new treatment, e.g., a drug. The re-\n",
      "searchers need to list all the main elements re-\n",
      "quiredforFDAregulation,suchasthetitle,pro-\n",
      "posed treatment, target condition, primary out-\n",
      "comemeasurements,eligibilitycriteria,etc.\n",
      "• NatureofSummaries: Aneffectivesummaryof\n",
      "clinicaltrialsneedtodeliverthemainmessage\n",
      "aboutthemotivationofthestudyaswellasthe\n",
      "route planning to reach the target. To make a\n",
      "goodsummaryofclinicaltrials,themodelneeds\n",
      "acomprehensiveviewof thewholedocuments\n",
      "andmaintainthekeyinformation.\n",
      "• Usage: We will use the “brief summary\" sec-\n",
      "tion written by human experts provided in the\n",
      "rawclinicaltrialdocumentsasthetargetforall\n",
      "models.\n",
      "D InterpretabilityofTriSum\n",
      "Document\n",
      "____________________________________\n",
      "____________________________________\n",
      "____________________________________\n",
      "____________________________________\n",
      "Aspect Extraction\n",
      "Essential Aspects\n",
      "Triple Extraction\n",
      "Detailed Triples\n",
      "head relation tail head relation tail head relation tail\n",
      "Summary\n",
      "________________________________\n",
      "________________________________\n",
      "elanoitaR\n",
      "of TriSum, illustrated in Figure 9, is designed\n",
      "withthistransparencyinmind.\n",
      "Startingwithagivendocument,TriSumiden-\n",
      "tifies its essential aspects. This step offers a\n",
      "clear insight into what the model perceives as\n",
      "the primary themes or topics within the docu-\n",
      "ment. Subsequently,usingtheseaspectsasanchors,\n",
      "TriSumrevisitsthedocumenttometiculouslyex-\n",
      "tract triples, structured as head | relation | tail ,\n",
      "⟨ ⟩ foreachaspect. Thesetriplesprovideastructured,\n",
      "detailedrepresentation,offeringgranularinsights\n",
      "intothemodel’sunderstandingoftherelationships\n",
      "and entities in the text. Finally, TriSum fuses\n",
      "these extracted aspects and triples to produce a\n",
      "summary. Bycorrelatingthefinalsummarywith\n",
      "thepreviouslyidentifiedaspectsandtriples,users\n",
      "can trace back the origins of particular summary\n",
      "fragments, gaining a clear understanding of how\n",
      "TriSumprocessesandabstractsinformation.\n",
      "Thisstep-by-stepelucidationofthesummariza-\n",
      "tion process significantly enhances the model’s\n",
      "transparency,makingitsdecision-makingrationale\n",
      "morediscernibleandhencefosteringtrustamong\n",
      "itsusers.\n",
      "E HyperparameterTuning\n",
      "Hyperparameter Values\n",
      "GoldenRationaleSelection\n",
      "ϕα {0.2,0.4,0.6,0.8,1.0,1.2}\n",
      "ϕβ {0.4,0.6,0.8,1.0,1.3,1.5,2.0}\n",
      "λcs {0.5,1.0,1.5,2.0}\n",
      "LDAlatenttopics {50,100,200,300,500,1000,3000,5000}\n",
      "RationaleLearning\n",
      "(λR,λS) {(1.0,1.0),(0.8,1.2),(0.5,1.5),(0.3,1.7)}\n",
      "Table4: HyperparametersofTriSumwetuned. We\n",
      "highlighttheoptimalonesbasedonourexperimentsin\n",
      "bold.\n",
      "Table4showsourcomprehensivehyperparame-\n",
      "terstudytoselecttheoptimalvaluesforTriSum.\n",
      "Summary Generation F CaseStudies\n",
      "InadditiontoFigure3,Figure10showsothertwo\n",
      "examplescomparingourTriSum’sperformance\n",
      "with our backbone model BART on XSum and\n",
      "Figure9: AbstractivesummarizationwithTriSum. ClinicalTrialdatasets. Wecandrawthefollowing\n",
      "Differentcolorsindicatedifferentessentialaspectscov-\n",
      "findings:\n",
      "ered by the document. We showcase how an aspect-\n",
      "triplerationaleisextractedandcontributetothefinal\n",
      "F.1 CaseStudyonXSum\n",
      "summarygeneration.\n",
      "In the given example, we juxtapose the perfor-\n",
      "Interpretabilityisparamountinunderstandingand manceofourapproach,TriSum,withBART,our\n",
      "trustingAIsystems,especiallyintaskslikeabstrac- backbone model. Upon scrutinizing the sourced\n",
      "tivesummarizationwherethederivationofconclu- articledetailingaresearchstudyonjobdiscrimina-\n",
      "sionsisn’talwaysovertlyapparent. Theworkflow tionagainstwomenwithTurkishnamesandthoseArticle:\n",
      "A university researcher sent 1,500 identical CVs to German firms -except that some bore the name MeryemOzturk and others the name Sandra Bauer. In 18.8% of cases Sandra\n",
      "Bauer was invited for interview, whereas the figure for Meryemwas just 13.5%. When the photo of Meryemshowed her in a headscarf only 4.2% invited her. The study was published\n",
      "by the Institute for the Study of Labour, in Bonn. The researcher was Doris Weichselbaumerfrom the University of Linz, in Austria. The findings are especially significant in light of\n",
      "Germany's current efforts to integrate record numbers of Muslim migrants, many of them refugees from the conflicts in Syria, Iraq and Afghanistan. More than a million asylum\n",
      "seekers reached Germany last year, and there has been a backlash from nationalist groups, especially Pegidaand Alternative for Germany (AfD). With an estimated three million\n",
      "people of Turkish origin, Germany is home to the largest Turkish diaspora. In her study, MsWeichselbaumersaid that \"a very modern binding of the headscarf was chosen to signal\n",
      "that the applicant was a young, modern woman who could easily fit into a secular environment\". It is normal in Germany to attacha photo to a job application, she pointed out. The\n",
      "result of the experiment \"implies that the candidate with the headscarf had to send 4.5 times as many applications as an identical applicant with a German name and no headscarf to\n",
      "receive the same number of callbacks for interview\", she said.…\n",
      "Ground truth summary: TriSumrationale:\n",
      "An experiment in Germany has found evidence of job discrimination <rationale> <aspects> Job discrimination against women with Turkish names in Germany. Job\n",
      "against women with Turkish names -and even more if they wear an discrimination against women wearing an Islamic headscarf in Germany. <triples> [women with\n",
      "Islamic headscarf. Turkish names | face discrimination in | job applications in Germany], [women wearing an Islamic\n",
      "headscarf | face even more discrimination in | job applications in Germany].\n",
      "BART summary: TriSumsummary:\n",
      "Women who apply for jobs in Germany wearing headscarves are Women with Turkish names in Germany face discrimination in job applications, and this\n",
      "much less likely to be invited for interview, a study suggests. discrimination is intensified for women wearing an Islamic headscarf.\n",
      "Article:\n",
      "Common indications for adult tonsillectomy include tonsillar hypertrophy, obstructive sleep apnea, and chronic tonsillitis. Although there are many studies looking into how best to\n",
      "prevent and treat post-operative pain and nausea, there are very few that give recommendations to the patients pre-operatively. This study's aim is to evaluate if pre-operative oral\n",
      "hydration has an impact in reducing post-operative pain, nausea and vomiting after adult tonsillectomies. If pre-operative hydration can reduce the pain associated with the procedure,\n",
      "then this could also be an adjunct to help reduce the amount of narcotic pain medication that is required.~Thegoal of this study is to evaluate if the amount of pre-operative oral\n",
      "hydration can reduce the amount of pain and nausea/vomiting in patients undergoing adult tonsillectomies. This study will be measuring post-operative pain and nausea scores at\n",
      "several points after the adult tonsillectomy procedure for participants that have consumed different amounts of clear fluid in the 24 hours leading up to their nothing by mouth (NPO)\n",
      "status. Participants will be randomized to one of three categories of preoperative fluid: 0.5 -1.5 Liters (L), 1.5 -3 L, and 3 -4.5 L, which will correlate to the low, medium and high fluid\n",
      "intake groups, respectively. Participants will be given a 1000 mL measuring water bottle that they can use to accurately record their fluid consumption inthe 24 hours prior to their\n",
      "pre-operative NPO status. The fluid consumption that they record will include all liquids (i.e., water, flavored water, juice, coffee, tea, soda, milk, alcohol). They will be asked to keep a\n",
      "record of the volume of caffeinated beverage versus un-caffeinated beverages.~Aftersurgery the participants will be given a form to record their pain and nausea scores. Participants\n",
      "will record their pain and nausea by using two separate 10-point scales at 7 different points after the procedure. These scores will be measured immediately after surgery, and at 4\n",
      "hours, 8 hours, 24 hours, 72 hours, 5 days and 7 days postoperative. This form, along with a count of opioid medication used, will be retuned to the physician at the first post-operative\n",
      "follow-up visit.\n",
      "Ground truth summary: TriSumrationale:\n",
      "Tonsillectomy is commonly described as one of the most painful procedures that <rationale> <aspects> Purpose of the study. Concern about tonsillectomy pain. Role of pre-\n",
      "an adult can undergo. This study's aim is to evaluate if pre-operative oral operative oral hydration. Randomization of participants into fluid intake groups. Recording of\n",
      "hydration has an impact in reducing post-operative pain, nausea and vomiting, fluid intake prior to surgery. Post-operative recording of pain and nausea. Recording of opioid\n",
      "and opioid medication use after adult tonsillectomies. Participants will be medication use. <triples> [Study | aims to| evaluate impact of pre-operative hydration on post-\n",
      "randomized to a low, medium and high fluid intake groups and will record fluid operative outcomes], [Tonsillectomy | is described as| painful procedure for adults], [Participants\n",
      "intake on the day before surgery. After surgery, participants will record their | will be randomized to | fluid intake groups: low, medium, high], [Participants | will record |\n",
      "pain and nausea at timepoints up to 7 days post-operative.Medication use will fluid intake before surgery], [Participants | will record | pain and nausea post-operatively],\n",
      "be recorded up to the first post-operative follow-up visit with their physician. [Participants | will record | opioid medication use until first follow-up].\n",
      "TriSumsummary:\n",
      "This study focuses on the possible influence of pre-operative oral hydration on post-operative\n",
      "pain, nausea, vomiting, and the use of opioid medication. Individuals taking part in the study will\n",
      "BART summary: be divided into low, medium, and high fluid intake categories. On the day before their procedure,\n",
      "The goal of this study is to evaluate if the amount of pre-operative oral they will note down their fluid consumption. After undergoing the tonsillectomy, these\n",
      "hydration can reduce the amount of pain and nausea/vomiting in patients participants will monitor and record their pain, nausea for up to seven days, and alsotheir opioid\n",
      "undergoing adult tonsillectomies. medication intake until their initial post-operative check-up with the doctor.\n",
      "Figure10: ExamplesofabstractivesummarizationonXSum(above)andClinicalTrial(below)datasets. We\n",
      "comparethesummarygeneratedbyourTriSumapproachtotheground-truthsummaryandtheonegeneratedby\n",
      "BART.Weusedifferentcolorstoshowthedistincttopicsinthearticleandsummary.\n",
      "wearingIslamicheadscarvesinGermany,wedis- informationissidestepped.\n",
      "cerndistinctnuancesinthesummariesrenderedby Moreover,TriSum’ssummarydoesn’tmerely\n",
      "bothmethods. reportthefindingsbutemphasizestheintensifica-\n",
      "BART’s summary encapsulates a broad under- tionofdiscriminationwhenbothfactors-aTurk-\n",
      "standing, highlighting that women wearing head- ishnameandanIslamicheadscarf-arecombined.\n",
      "scarves in Germany are at a disadvantage during Suchalayeredinsightisinvaluable,especiallyin\n",
      "jobapplications. Whileitsuccessfullyconveysa sensitive subjects such as discrimination, where\n",
      "salient point, it omits the specific discrimination capturingtheentirescopeoftheissueiscrucial.\n",
      "againstwomenwithTurkishnames. In essence, while BART gives a generalized\n",
      "overview, TriSum offers a richer, more compre-\n",
      "TriSum, on the other hand, demonstrates its\n",
      "hensivenarrativethatmirrorsthedepthandbreadth\n",
      "prowessthroughamoreholistic,nuanced,andde-\n",
      "oftheoriginalarticle,underscoringthestrengthand\n",
      "tailed summary. It distinctly notes both aspects\n",
      "precisionofourapproach.\n",
      "of the discrimination: one against women with\n",
      "Turkishnamesandtheotheragainstthosedonning\n",
      "F.2 CaseStudyonClinicalTrial\n",
      "anIslamicheadscarf. TriSum’srationalesection\n",
      "further accentuates its strength by explicitly pre- In this case study centered around adult tonsil-\n",
      "sentingthecoreaspectsandtriplesthatdelineate lectomies, it is evident that the BART primarily\n",
      "thefocuspointsofthesummary. Thismethodical graspedthecoregoalofthestudybutmissedouton\n",
      "extraction and representation ensure that no vital essentialdetails,particularlythevariedfluidintakegroups and post-operative data recording. Mean- ClinicalTrial-Base\n",
      "while,thegroundtruthsummaryoffersacompre- Model R-1 R-2 R-L ∆ BS BAS\n",
      "hensiveview,butitremainsrelativelygeneralized. Baselines\n",
      "Thestrengthofourapproach,theaspect-triple\n",
      "T5Large 53.9 41.7 47.2 −2.0% 90.49 -1.91\n",
      "BARTLarge 51.8 38.6 43.6 +4.4% 89.61 -1.99\n",
      "rationaled summarization (TriSum), is signifi- PEGASUS 51.8 40.7 44.8 +1.9% 90.16 -1.61\n",
      "cantlyhighlightedwhenwedelveintothedetails\n",
      "GPT-3.5zero-shot 45.4 23.8 32.5 +37.6% 89.00 -2.44\n",
      "OurMethod\n",
      "and the rationale-driven structure it adheres to.\n",
      "GPT-3.5TriSum 54.1 37.6 42.2 +4.5% 90.84 -1.52\n",
      "TriSumoperatesbyidentifyingessentialaspects TriSum-S 53.6 42.2 46.6 −1.8% 90.67 -1.66\n",
      "TriSum-C 50.3 37.2 42.8 +7.4% 89.25 -2.14\n",
      "ofthetext,followedbyextractingandconstructing\n",
      "TriSum-J 52.9 41.8 45.2 — 90.81 -1.64\n",
      "triplesthatmaptherelationshipsinthecontent.\n",
      "Table6: PerformancecomparisonofROUGEScores\n",
      "• Aspect-DrivenUnderstanding: TriSum’sra- andsemanticsimilarityscoresonClinicalTrial-Base\n",
      "tionale points out the key aspects such as the Dataset.Thetop-3resultsarehighlighted. Ourback-\n",
      "bonemodel,BART ,isshadowedforreference.\n",
      "purposeofthestudy,concernsrelatedtotonsil- Large\n",
      "lectomypain,theroleofpre-operativehydration,\n",
      "among others. By capturing these aspects, the G.2 MoreBaselinesandContrastiveLearning\n",
      "modelsetsthestageforasummarythatdoesnot FrameworkAdaptation\n",
      "missoutonthediverseelementsoftheoriginal\n",
      "text.\n",
      "CNN/DailyMail\n",
      "• Triple-Based Detail Extraction: The aspect- Model R-1 R-2 R-L BS BAS\n",
      "BART 44.0 21.1 40.6 87.98 -3.45\n",
      "drivenapproachisfurtherenrichedbythetriples Large\n",
      "BART 44.2 21.2 40.9 88.04 -3.47\n",
      "12-6-SFT\n",
      "TriSum generates. Thesetriples,suchas[Par- PLATE BART12-12,λ=2.0 44.9 22.0 41.4 88.12 -3.34\n",
      "BRIO-Mul 47.6 23.5 44.5 88.74 -3.22\n",
      "ticipants | will record | pain and nausea post- BART\n",
      "LLAMA-2 36.4 14.2 30.4 87.84 -3.31\n",
      "zero-shot\n",
      "operatively], ensure that the summary remains\n",
      "TriSum+BRIO 48.0 24.4 45.3 89.38 -3.07\n",
      "Mul\n",
      "faithfultothearticlebycapturingnuancedrela- TriSum 45.5 22.7 42.0 88.62 -3.28\n",
      "LLAMA-2\n",
      "tionships. Itdoesnotjustreiteratewhatthestudy XSum\n",
      "does,butalsohowitgoesaboutit,ensuringthe Model R-1 R-2 R-L BS BAS\n",
      "BART 45.4 22.3 37.3 91.62 -2.50\n",
      "readerunderstandsthemethodology. Large\n",
      "BART 44.8 22.2 37.1 91.55 -2.56\n",
      "12-3-KD\n",
      "PLATE BART12-12,λ=1.5 45.3 22.3 37.2 91.60 -2.52\n",
      "• Precision and Brevity: The TriSum sum- BRIO-Mul 47.1 23.5 38.2 91.98 -2.40\n",
      "BART\n",
      "marycapturesallthekeypoints—rightfromthe LLAMA-2 zero-shot 30.2 10.4 22.3 89.12 -2.53\n",
      "study’sfocus,thecategorizationofparticipants, TriSum+BRIO Mul 48.2 25.3 39.9 92.43 -2.21\n",
      "TriSum 47.2 24.4 39.3 92.12 -2.35\n",
      "LLAMA-2\n",
      "tothepost-operativedocumentation—withoutbe-\n",
      "Table7: Additionalexperiments.\n",
      "comingverbose. Itoffersacondensedyetcom-\n",
      "prehensiveviewofthearticle,ensuringthatread-\n",
      "In addition to Table 2 and 3, we further tested\n",
      "erscanquicklygraspthecoreconceptswithout\n",
      "baselinesBART (ShleiferandRush,2020)\n",
      "gettingoverwhelmed. 12-3-KD\n",
      "and PLATE (Zhang et al., 2022),\n",
      "BART12-12,λ=1.5\n",
      "G AdditionalEvaluation a general contrastive learning-based framework\n",
      "BRIO-Mul (Liuetal.,2022),andanotherlead-\n",
      "BART\n",
      "G.1 PerformanceonClinicalTrial-Base\n",
      "ing LLM LLAMA-2-70B (Touvron et al., 2023)\n",
      "InadditiontotheClinicalTrial(Large)dataset,we on CNNDM and XSum datasets. We also tested\n",
      "alsoconstructedasimplerversion-ClinicalTrial- TriSum-J(trainedbyGPT-3.5rationale)futher\n",
      "Base where we consider the article-summary trained with contrastive learning strategy from\n",
      "pairs included in this dataset to be those with a BRIO, denoted as “TriSum+BRIO ”. For a\n",
      "Mul\n",
      "BARTScore higher than 2.0. The statistics for fair comparison, we use BART as backbone of\n",
      "−\n",
      "thisdatasetareinTable5shownasfollows. BRIO for both datasets, while original paper of\n",
      "BRIO uses Pegasus for XSum. Moreover, we re-\n",
      "#Samples #Words\n",
      "portTriSumtrainedwiththe“aspect-triple”ratio-\n",
      "Dataset Train Valid Test Doc. Sum.\n",
      "ClinicalTrial-Base 62,012 7,752 7,752 277.7 76.1 nalesgeneratedbyLLAMA-2-70B.Wecouldnot\n",
      "testwithGPT-4’srationalesduetotheexpensive\n",
      "Table5: StatisticsofClinicalTrial-Base.\n",
      "APIcost. Table7presentsourfindings: (1)BRIO,\n",
      "OurevaluationresultsareshowninTable6below. asageneralcontrastivelearningframework,canbeadaptedbyTriSumandimproveitsperformance,\n",
      "achievingSOTAresults;(2)Inazero-shotscenario,\n",
      "LLAMA-2-70B outperforms GPT-3.5 on XSum;\n",
      "(3)TriSumshowscomparableperformancewith\n",
      "both LLAMA-2 and GPT-3.5 rationales on the\n",
      "datasets.\n",
      "G.3 FatualnessImprovementwithTriSum\n",
      "BART Trisum-J GPT-3.5zero-shot GPT-3.5TriSum\n",
      "FACTSCORE 88.1 92.9 85.3 93.7\n",
      "Table8: FactualconsistencyevaluationonCNNDM\n",
      "test set. Results will not affect the original paper’s\n",
      "contributions.\n",
      "We tested the Factual Consistency (FC) by\n",
      "FACTSCORE (Minetal.,2023)withtheirNPset-\n",
      "tingusingInst-LLAMA,andwiththesourcetextas\n",
      "theknowledgesource. Table8showsthatTriSum\n",
      "cansubstantiallyenhanceFC,especiallywhenus-\n",
      "ing its rationale for GPT-3.5 prompting. This is\n",
      "becausetriplesemphasizesthefactscontainedin\n",
      "the source text. The result also indicates that, by\n",
      "systematicallyextractingthe“aspect-triple”ratio-\n",
      "nale,themodelestablishesastructuredframework\n",
      "thatconstrainsthegenerationprocess,minimizing\n",
      "the likelihood of generating content unsupported\n",
      "bythesourcetext.\n"
     ]
    }
   ],
   "source": [
    "print(paper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed525556-b987-41a9-9178-87ab0baeab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"gpt-3.5-turbo-0125\"\n",
    "# MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "MODEL_NAME = \"gpt-4-0125-preview\"\n",
    "TEMPERATURE = 0.7\n",
    "# OpenAIクライアントの初期化\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e3b4035-35c4-4343-ba98-68f91a6da001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_summary(model_name, doc):\n",
    "    # 文書を要約するプロセスにおける、ゼロショット設定での要約生成のタスク\n",
    "    prompt = [{'role': 'system', 'content': \"Given a document, summarize the document in\"}]\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : 'one sentence: Given a document, summarize the document in'})\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : f'three sentence: Document: {doc}'})\n",
    "    prompt.append({\"role\": \"user\", \"content\": f\"Summary:\"})\n",
    "    \"\"\"\n",
    "    文書を与えられた場合、その文書を1文で要約します。\n",
    "    文書を与えられた場合、その文書を3文で要約します。\n",
    "    文書: {doc}\n",
    "    要約:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 概要と提案手法名抽出用のプロンプトテンプレートを作成\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, # model = \"deployment_name\".\n",
    "        messages=prompt,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    summary_str = response.choices[0].message.content\n",
    "    \n",
    "    return summary_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03a13201-11fc-4952-8970-cf704b24876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Rationale Probing 11.475[s]\n"
     ]
    }
   ],
   "source": [
    "# LLM理由付け探索 (LLM Rationale Probing)\n",
    "with Timer(prefix=\"LLM Rationale Probing\"):\n",
    "    gt_summary = ground_truth_summary(MODEL_NAME, paper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4596d0e-72f0-4e7a-a262-95719a93f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriSum is a framework designed to distill text summarization capabilities from large language models (LLMs) into smaller, local models, enhancing performance and interpretability across various datasets. Through a three-step process involving rationale probing, golden rationale selection, and curriculum learning, TriSum systematically improves summarization quality while offering insights into the rationale behind model decisions. Extensive testing across datasets like CNN/DailyMail, XSum, and ClinicalTrial demonstrates TriSum's superiority over state-of-the-art models and its adaptability to different summarization tasks. Additionally, TriSum's methodology of utilizing aspect-triple rationales not only aids in generating more accurate and factually consistent summaries but also highlights the potential for scaling this approach to other summarization models and tasks.\n"
     ]
    }
   ],
   "source": [
    "print(gt_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16d557a-1d42-4438-a9f0-8952a5408ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationale_probing(model_name, doc, gt_summary):\n",
    "    # TriSum: LLM理由付け探索 (LLM Rationale Probing)\n",
    "    # 与えられた文書に基づいて重要な側面とそれらに関連する三重項（エンティティとその関係）を抽出し、これらを使用して要約を生成\n",
    "    prompt = [{'role': 'system', 'content': \"Given a document and its ground-truth summary, do\"}]\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : 'the following tasks:'})\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : '(1) According to the Ground-truth Summary, extract essential aspects of the Document.'})\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : '(2) For each essential aspect, retrieve detailed triples in the format [ENTITY1 | RELATION | ENTITY2] used to compose the ground-truth summary.'})\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : '(3) With the retrieved triples, compose a summary.The essential aspects, triples, and composed summary should be in the same response, separated by a new line.'})\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : 'All triples [ENTITY1 | RELATION | ENTITY2] should be in length 3 (separated by \"|\").'})\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : 'Example:\\n\\n================Example=================\\nPrompt:\\n[Document]: [document]\\n[Ground-truth Summary]: [ground-truth summary]\\nUpdate:\\nEssential Aspects:\\n[aspects]\\nTriples:\\n- [ENTITY1_1 | RELATION_1 | ENTITY1_2]\\n- [ENTITY2_1 | RELATION_2 | ENTITY2_2]\\n- [ENTITY3_1 | RELATION_3 | ENTITY3_2]\\n- ...\\nGenerated Summary:\\n[summary]\\n========================================'})\n",
    "    \n",
    "    prompt.append({\"role\": \"system\", \"content\": \"Please format the output in Markdown.\"})\n",
    "    prompt.append({\"role\": \"system\", \"content\": \"Results must be in Japanese.\"})\n",
    "    \n",
    "    prompt.append({\"role\": \"user\", \"content\": f\"Prompt:\"})\n",
    "    prompt.append({\"role\": \"user\", \"content\": f\"[Document]: {doc}\"})\n",
    "    prompt.append({\"role\": \"user\", \"content\": f\"[Ground-truth Summary]: {gt_summary}\"})\n",
    "    prompt.append({\"role\": \"user\", \"content\": f\"Update:\"})\n",
    "    \"\"\"\n",
    "    システム\n",
    "\n",
    "    文書とその正確な要約が与えられた場合、以下のタスクを実行してください。\n",
    "    (1) 正確な要約に基づき、文書の重要な側面を抽出します。\n",
    "    (2) それぞれの重要な側面について、正確な要約を構成するために使用された[ENTITY1 | RELATION | ENTITY2]の形式で詳細な三つ組を取得します。\n",
    "    (3) 取得した三つ組を用いて、要約を作成します。重要な側面、三つ組、および作成した要約は、新しい行で区切られ、同じ返答の中に含めてください。\n",
    "    全ての三つ組[ENTITY1 | RELATION | ENTITY2]は、3つの長さである必要があります（\"|\"で区切られる）。\n",
    "    例:\n",
    "    ================例=================\n",
    "    プロンプト:\n",
    "    [文書]: [document]\n",
    "    [正確な要約]: [ground-truth summary]\n",
    "    更新:\n",
    "    重要な側面:\n",
    "    [aspects]\n",
    "    三つ組:\n",
    "\n",
    "    [ENTITY1_1 | RELATION_1 | ENTITY1_2]\n",
    "    [ENTITY2_1 | RELATION_2 | ENTITY2_2]\n",
    "    [ENTITY3_1 | RELATION_3 | ENTITY3_2]\n",
    "    ...\n",
    "    生成された要約:\n",
    "    [summary]\n",
    "    ========================================\n",
    "    プロンプト:\n",
    "    [文書]: {doc}\n",
    "    [正確な要約]: {gt_summary}\n",
    "    更新:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 概要と提案手法名抽出用のプロンプトテンプレートを作成\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name, # model = \"deployment_name\".\n",
    "        messages=prompt,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    summary_method_name_str = response.choices[0].message.content\n",
    "    \n",
    "    return summary_method_name_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec0af9df-d62d-4fca-a67c-ba98fb8faa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Rationale Probing 41.289[s]\n"
     ]
    }
   ],
   "source": [
    "# LLM理由付け探索 (LLM Rationale Probing)\n",
    "with Timer(prefix=\"LLM Rationale Probing\"):\n",
    "    # summary_method_name = rationale_probing(MODEL_NAME, paper_text, info[\"summary\"])\n",
    "    summary_method_name = rationale_probing(MODEL_NAME, paper_text, gt_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f1c3892-9369-4f07-9138-b72512639fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Essential Aspects:\\n- TriSum framework\\n- Distillation process from LLMs to local models\\n- Performance and interpretability enhancement\\n- Three-step process: rationale probing, golden rationale selection, curriculum learning\\n- Testing across CNN/DailyMail, XSum, and ClinicalTrial datasets\\n- Superiority over state-of-the-art models\\n- Utilization of aspect-triple rationales for accuracy and factual consistency\\n\\nTriples:\\n- [TriSum | is designed to | distill text summarization capabilities]\\n- [TriSum | enhances | performance and interpretability]\\n- [Process | involves | rationale probing, golden rationale selection, curriculum learning]\\n- [TriSum | tested across | CNN/DailyMail, XSum, and ClinicalTrial]\\n- [TriSum | demonstrates | superiority over state-of-the-art models]\\n- [TriSum | utilizes | aspect-triple rationales for accuracy and factual consistency]\\n\\nGenerated Summary:\\nTriSumは、大規模な言語モデル（LLM）から小規模なローカルモデルへのテキスト要約能力の蒸留を目的としたフレームワークであり、CNN/DailyMail、XSum、ClinicalTrialなどのさまざまなデータセットでのパフォーマンスと解釈可能性を向上させます。理由の探求、黄金の理由の選択、カリキュラム学習という3つのステップを通じて、TriSumは要約品質を体系的に改善し、モデルの決定背後にある理由に洞察を提供します。また、アスペクト-トリプルの理由を利用することで、より正確で事実に基づいた要約を生成するだけでなく、このアプローチを他の要約モデルやタスクにスケーリングする可能性を示しています。'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_method_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9613f710-024a-4727-9bf8-e17818c4ca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essential Aspects:\n",
      "- TriSum framework\n",
      "- Distillation process from LLMs to local models\n",
      "- Performance and interpretability enhancement\n",
      "- Three-step process: rationale probing, golden rationale selection, curriculum learning\n",
      "- Testing across CNN/DailyMail, XSum, and ClinicalTrial datasets\n",
      "- Superiority over state-of-the-art models\n",
      "- Utilization of aspect-triple rationales for accuracy and factual consistency\n",
      "\n",
      "Triples:\n",
      "- [TriSum | is designed to | distill text summarization capabilities]\n",
      "- [TriSum | enhances | performance and interpretability]\n",
      "- [Process | involves | rationale probing, golden rationale selection, curriculum learning]\n",
      "- [TriSum | tested across | CNN/DailyMail, XSum, and ClinicalTrial]\n",
      "- [TriSum | demonstrates | superiority over state-of-the-art models]\n",
      "- [TriSum | utilizes | aspect-triple rationales for accuracy and factual consistency]\n",
      "\n",
      "Generated Summary:\n",
      "TriSumは、大規模な言語モデル（LLM）から小規模なローカルモデルへのテキスト要約能力の蒸留を目的としたフレームワークであり、CNN/DailyMail、XSum、ClinicalTrialなどのさまざまなデータセットでのパフォーマンスと解釈可能性を向上させます。理由の探求、黄金の理由の選択、カリキュラム学習という3つのステップを通じて、TriSumは要約品質を体系的に改善し、モデルの決定背後にある理由に洞察を提供します。また、アスペクト-トリプルの理由を利用することで、より正確で事実に基づいた要約を生成するだけでなく、このアプローチを他の要約モデルやタスクにスケーリングする可能性を示しています。\n"
     ]
    }
   ],
   "source": [
    "print(summary_method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "881a7a31-43c6-4b3d-bdc8-433c76a2520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_method_algorithm(model_name, text, method_name):\n",
    "    # アルゴリズムの説明\n",
    "    prompt = [{'role': 'system', 'content': \"Please explain the algorithm of the method name from the following text in detail, using both sentences and formulas. Carefully describe the mechanism in order, ensuring that it can be understood and implemented. Design the process flow in a way that allows for the algorithm to be implemented without any omissions or excess.\"}]\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : \"Describe the algorithm in detail, explaining what it aims to achieve, how it processes to accomplish this, and how exactly these processes are carried out, regardless of the length of the explanation. Just ensure it is accurate.\"})\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : \"Outputs should be generated in step by step.\"})\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : \"Please format the output in Markdown.\"})\n",
    "    prompt.append({\"role\": \"system\", \"content\": \"Results must be in Japanese.\"})\n",
    "    prompt.append({\"role\": \"system\", \"content\": 'Please generate a JSON from the following input text. Use \"method\" as the schema, and for the key, use \"the detailed explanation of the processing of the method_name algorithm in simple language\". Generate it in the format of {\"method\": \"the result of a detailed explanation of the method_names algorithm described in simple language\"}.'})\n",
    "    \n",
    "    prompt.append({\"role\": \"user\", \"content\": 'Generate a JSON from the following input text. Use \"method\" as the schema, and use the judgment result as the key, to create it in the format {\"method\": the result of grouping the search_query based on relevance into a list format that can be used in Python}.'})\n",
    "    \n",
    "    prompt.append({\"role\": \"user\", \"content\": f\"Input text: {text}\"})\n",
    "    prompt.append({\"role\": \"user\", \"content\": f\"method name: {method_name}\"})\n",
    "    \n",
    "    \"\"\"\n",
    "    システム\n",
    "    あなたは以下の text から method name のアルゴリズムを順番に過不足なく文章と数式で丁寧に順番に仕組みが理解でき、実装をするための処理の流れを設計できるように説明してください。\n",
    "    アルゴリズムの説明は、何を実現するために、どのように処理を実行し、その処理はどのように実行されるのかをどれだけ長くなってもよいのでとにかく正確に説明してください。\n",
    "    出力は Markdown 形式にしてください。\n",
    "    \n",
    "    結果は日本語でなければならない。\n",
    "    以下の入力テキストからJSONを生成してください。スキーマには \"method\"、キーには\"text から method_name のアルゴリズムを平易な文章で処理内容を詳細に説明した内容\"を使ってください。\"method\": \"method_name のアルゴリズムを平易な文章で処理内容を詳細に説明した内容した結果}'}の形式で生成してください。\n",
    "    user\n",
    "    \n",
    "    入力されたテキスト: {text}\n",
    "    method name: {method_name}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 概要と提案手法名抽出用のプロンプトテンプレートを作成\n",
    "    method = client.chat.completions.create(\n",
    "        model=model_name, # model = \"deployment_name\".\n",
    "        messages=prompt,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    method_str = method.choices[0].message.content\n",
    "    \n",
    "    # JSON形式の文字列を辞書に変換\n",
    "    method_algorithm = json.loads(method_str)\n",
    "    \n",
    "    # 出力と新しいメッセージをステートに反映\n",
    "    return {\n",
    "        \"method\": method_algorithm[\"method\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57774760-481b-404d-bd35-f356b62d1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_method_algorithm 26.765[s]\n"
     ]
    }
   ],
   "source": [
    "with Timer(prefix=\"explain_method_algorithm\"):\n",
    "    # method_algorithm = explain_method_algorithm(MODEL_NAME, paper_text, str(summary_method_name[\"method_name\"]))\n",
    "    method_algorithm = explain_method_algorithm(MODEL_NAME, paper_text, summary_method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f166eba-0a18-42da-bbd0-d6ff65f7d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriSumは、大規模な言語モデル（LLM）から小規模なローカルモデルへのテキスト要約能力の蒸留を目的としたフレームワークであり、CNN/DailyMail、XSum、ClinicalTrialなどのさまざまなデータセットでのパフォーマンスと解釈可能性を向上させます。理由の探求、黄金の理由の選択、カリキュラム学習という3つのステップを通じて、TriSumは要約品質を体系的に改善し、モデルの決定背後にある理由に洞察を提供します。また、アスペクト-トリプルの理由を利用することで、より正確で事実に基づいた要約を生成するだけでなく、このアプローチを他の要約モデルやタスクにスケーリングする可能性を示しています。\n"
     ]
    }
   ],
   "source": [
    "print(method_algorithm[\"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b16514f0-9f90-4792-ae3c-9f68b10a62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudocode_for_method(model_name, algorithm):\n",
    "    # アルゴリズムの説明\n",
    "    prompt = [{'role': 'system', 'content': \"Based on the description of the following algorithm, please create a comprehensive pseudo-implementation code in Python without omitting any details.\"}]\n",
    "    prompt.append({\"role\" : \"system\", \"content\" : \"Outputs should be generated in step by step.\"})\n",
    "    prompt.append({\"role\": \"system\", \"content\": \"Please format the output in Markdown.\"})\n",
    "    prompt.append({\"role\": \"system\", \"content\": \"Comment must be in Japanese.\"})\n",
    "    prompt.append({\"role\": \"system\", \"content\": 'Please generate a JSON from the following input text. Use \"code\" as the schema, and for the key, use \"the result of generating code that executes the algorithm of algorithm in Python\". Generate it in the format of {\"code\": \"the result of reproducing the algorithm algorithm in Python code\"}.'})\n",
    "        \n",
    "    prompt.append({\"role\": \"user\", \"content\": f\"algorithm: {algorithm}\"})\n",
    "    \n",
    "    \"\"\"\n",
    "    システム\n",
    "    あなたは以下の algorithm の説明を基にpythonの疑似実装コードを過不足なく作成してください。\n",
    "    出力は Markdown 形式にしてください。\n",
    "    \n",
    "    コメントは日本語でなければならない。\n",
    "    以下の入力テキストからJSONを生成してください。スキーマには \"code\"、キーには\"algorithm のアルゴリズムをpythonのコードで動くようにコードを生成した結果\"を使ってください。\"code\": \"algorithm のアルゴリズムをpythonのコードで再現した結果\"}'}の形式で生成してください。\n",
    "    user\n",
    "    \n",
    "    algorithm: {algorithm}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 概要と提案手法名抽出用のプロンプトテンプレートを作成\n",
    "    code_res = client.chat.completions.create(\n",
    "        model=model_name, # model = \"deployment_name\".\n",
    "        messages=prompt,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    code_str = code_res.choices[0].message.content\n",
    "    print(code_str)\n",
    "    \n",
    "    # JSON形式の文字列を辞書に変換\n",
    "    code = json.loads(code_str)\n",
    "    \n",
    "    # 出力と新しいメッセージをステートに反映\n",
    "    return {\n",
    "        \"code\": code[\"code\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d5978ae-25c5-4720-9c0f-bdb97a6f356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"code\": \"# TriSumアルゴリズムの実装\\n\\n# 必要なライブラリのインポート\\nimport numpy as np\\n\\n# TriSumフレームワークの概要\\n# TriSumは、大規模な言語モデルから小規模なローカルモデルへのテキスト要約能力の蒸留を目的としたフレームワークです。\\n# これは、CNN/DailyMail、XSum、ClinicalTrialなどのさまざまなデータセットでのパフォーマンスと解釈可能性を向上させます。\\n\\n# TriSumの3つの主要なステップ\\n\\n# 1. 理由の探求\\n# コメント: 大規模な言語モデルを使って、テキストから重要な理由を抽出します。\\n\\n# 2. 黄金の理由の選択\\n# コメント: 抽出した理由の中から最も重要なものを選択します。\\n\\n# 3. カリキュラム学習\\n# コメント: 選択した理由を使って、小規模なローカルモデルを段階的に訓練します。\\n\\n# コードの具体的な実装は、特定のモデルやデータセットに依存しますが、このフレームワークの概念は、\\n# 要約品質を体系的に改善し、モデルの決定背後にある理由に洞察を提供するために使用できます。\\n\\n# アスペクト-トリプルの理由を利用することで、より正確で事実に基づいた要約を生成することが可能になります。\\n# また、このアプローチは他の要約モデルやタスクにもスケーリングする可能性を示しています。\\n\\n# 注意: このコードは、TriSumフレームワークの概念と主要なステップを説明するものであり、\\n# 実際の実装には特定のモデルやデータセット、およびそれらの処理に関する詳細なコードが必要になります。\"\n",
      "}\n",
      "generate_pseudocode_for_method 36.577[s]\n"
     ]
    }
   ],
   "source": [
    "with Timer(prefix=\"generate_pseudocode_for_method\"):\n",
    "    code_method = generate_pseudocode_for_method(MODEL_NAME, str(method_algorithm[\"method\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f836ddfa-57a4-49e9-ab33-227c2f7ed3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TriSumアルゴリズムの実装\n",
      "\n",
      "# 必要なライブラリのインポート\n",
      "import numpy as np\n",
      "\n",
      "# TriSumフレームワークの概要\n",
      "# TriSumは、大規模な言語モデルから小規模なローカルモデルへのテキスト要約能力の蒸留を目的としたフレームワークです。\n",
      "# これは、CNN/DailyMail、XSum、ClinicalTrialなどのさまざまなデータセットでのパフォーマンスと解釈可能性を向上させます。\n",
      "\n",
      "# TriSumの3つの主要なステップ\n",
      "\n",
      "# 1. 理由の探求\n",
      "# コメント: 大規模な言語モデルを使って、テキストから重要な理由を抽出します。\n",
      "\n",
      "# 2. 黄金の理由の選択\n",
      "# コメント: 抽出した理由の中から最も重要なものを選択します。\n",
      "\n",
      "# 3. カリキュラム学習\n",
      "# コメント: 選択した理由を使って、小規模なローカルモデルを段階的に訓練します。\n",
      "\n",
      "# コードの具体的な実装は、特定のモデルやデータセットに依存しますが、このフレームワークの概念は、\n",
      "# 要約品質を体系的に改善し、モデルの決定背後にある理由に洞察を提供するために使用できます。\n",
      "\n",
      "# アスペクト-トリプルの理由を利用することで、より正確で事実に基づいた要約を生成することが可能になります。\n",
      "# また、このアプローチは他の要約モデルやタスクにもスケーリングする可能性を示しています。\n",
      "\n",
      "# 注意: このコードは、TriSumフレームワークの概念と主要なステップを説明するものであり、\n",
      "# 実際の実装には特定のモデルやデータセット、およびそれらの処理に関する詳細なコードが必要になります。\n"
     ]
    }
   ],
   "source": [
    "print(code_method[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4b29c-a23f-4188-9d87-42d417d6de76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
